Episode: 2658
Title: HPR2658: Questions on podcast production
Source: https://hub.hackerpublicradio.org/ccdn.php?filename=/eps/hpr2658/hpr2658.mp3
Transcribed: 2025-10-19 07:08:42

---

This is HPR episode 2658 entitled Question on Podcast Production.
It is posted by Al and in about 46 minutes long and carry my clean flag.
The summary is Al asks main questions about podcast production.
This episode of HPR is brought to you by an honesthost.com.
Get 15% discount on all shared hosting with the offer code HPR15.
That's HPR15.
Better web hosting that's honest and fair at An Honesthost.com.
Welcome to another exciting episode of Hacker Puberadio.
I'm Al, I'm Dave and we decided that because I calling for shows and that I wanted to ask
Dave, he does lots of podcasts and for the post production, I am a host of podcasts
called the Abnavon podcast and I wanted to ask Dave some questions about how the best
way to record the audio for a podcast and post production.
Thank you for doing the National Podcast post month which is annual thing in November
where you get to post or produce and post a podcast every day.
So I wanted to record or work out the best way of producing and making the best audio.
I mean reason why is that I always find it really difficult to kind of listen to bad
audio and podcast.
It can be a bit of a criticism for me because obviously on my Abnavon podcast in the early
days, the audio wasn't that great.
It wasn't down to me doing the audios, it was down to the guys who were doing it at
the other end but now Dave was a year picked up and it's a lot better.
So I wanted to kind of ask you some questions and see where we're going to go.
So what's your kind of take on the bad podcast audio?
Well to be perfectly honest, bad audio I think is one of the things that's going to
kill a podcast.
I will start by saying that with Admin Admin, it is not an easy task to get the audio
of a suitable quality or a suitable level between multiple participants where there is
just one of you.
So doing a solo podcast, you've only got to concern yourself about how you sound.
So when you're including one or more additional participants, then trying to make sure that
they're the same level that they are of the same relative quality, it does get a little
bit complex in that respect.
But audio on a podcast in general, I think as a rule, certain podcasts need to be smashed
over the head.
And I say that in the nicest possible way, but I've picked up in the last few months,
quite a few new podcasts in the true crime arena.
So I've picked up a new podcast called One I Open, which is hosted by Alaskan called
Steffi.
She's based here in the UK.
And the production quality on that podcast is incredibly good.
The audio's good, the background music is good and it's appropriate.
Her delivery style is pretty much spot on.
So as a podcast, it is quite enjoyable to listen to.
But through her show and through interactions with other people on Twitter, I've started
listening to other true crime podcasts as well.
And I'm not going to name any names, but some of the ones that I have picked up and have
started listening to.
I'm in the process of deciding to sack them off, because the quality of the audio is
not necessarily such of an issue, but the level is really, really difficult to hear.
Now I consume the majority of my podcasts in the car on the way to and from work.
And I have, it's a relatively old car, it's a little bit noisy.
But the fact that I have to have my phone volume cranked up to the maximum and my stereo
volume cranked up to the maximum in order to not be able to hear what people are saying
suggests that something is wrong.
Yeah, I know, I asked for one of my questions for everyone at the level and like, because
I don't want to have sometimes you have to turn up the phone volume really loud and the
and the car stereo right up to the max to get the kind of out of it.
I mean, no, I'll listen to some of self-help podcasts and like, one of my suggestions is
I'm a really good subject.
But the way they were recording this guest was like over the phone and it was just too
difficult to listen to.
It's a freedom level.
So I just basically bend that off straight away kind of thing.
Yeah, I think there's a little bit of confusion over what is the right level for individual
voices to be placed at.
Now there is something called loudness, which is nothing to do with peak level or volume,
is to do with how your ear perceives loudness.
So you could have, I mean, you've recorded stuff in Audacity before.
You know about the waveform that you get as you're recording.
And if you look at your Audacity now, as you're recording, you'll probably see that you're
I don't know about half the height of the full grid.
Yeah.
As you're talking.
Yeah, like now I'm talking.
I can see it's between 0.5 and minus 0.5.
Right.
Which is about perfect.
Now, you might think that that's quite quiet.
But when you're listening to it, you'll discover that it's a really nice reasonable level
to listen to, because it's not just about how loud it is or how amplified it is, but
it's about how the sound itself is formed and how it's positioned.
Now when I do podcast production, either for my own show, The Bugcast or For Abin Abin,
what I do is I put the audio through a service, which levels the loudness for you.
So that it is at a set level throughout the entirety of the recording.
If you record individuals, one who is relatively quiet and one who is relatively loud, if you
just amplify it with the Audacity, all you're doing is making the loud part louder and
the quiet part marginally loud us, you're still going to get that same difference of loud
and soft, loud and soft, loud and soft as you go through.
What this service does, which I'll talk about later, actually, it levels off the individual
parts, so it gives the perception that the loudness is the same across them.
It's a really quite a clever concept.
To be fair, it's not that difficult to use, so it is something I do recommend.
So maybe we could go through our podcasting setup first, our audio setup, and then we
could then talk about some more questions I have.
I've got a Q2U, I think it's the same mic as which you have it is, yes.
So it's basically, I got recommended when I started, it's got a USB on the back of
it, so when you plug it into your PC, it actually comes as an audio device, and it's also
got an XLR at the back of it, so in case in Feature, if you wanted to get a mixer, you can
plug it into a mixer, but once it goes into USB into my Linux machine, I've got a
basically on the mic, I'm in a pop filter, and it's normally just recorded into Audacity.
That's my little setup I have, say, what about you, Dave?
OK, so when you say pop filter, are you talking about the round circular thing that sits
in front of the microphone, or is it like a sock that goes over the end of the microphone?
No, like a round circle.
Gotcha.
OK, so my setup, as you've quite rightly said, I've got a Samsung QQ as well, but I'm
using it in XLR through to a mixer, which is connected to my studio PC through USB.
I've also got it on an arm with, let's say with a pop filter over the end of it, but
I've also got a shock mount on the microphone itself, meaning that if I tap the microphone,
you can hear it, but if I tap the boom arm that is connected to, you can't.
The idea being is that wherever you have your boom arm connected to, which is probably
likely to be the desk that you're actually sitting at, when you tap on the table, like
I just did, it doesn't come through the arm into the microphone, and this shock mount
was dirt cheap, I think it costs about Â£10 from Amazon, it's not the best thing in the
world, it is starting to fall apart a bit, but it still does its job quite nicely.
Yeah, oddly, I've got my arm hanging down from like a shelf above me, because it doesn't
really work, we have a way around for me. No, and I understand that. So the idea of putting
a shock mount in is probably unnecessary for you, because unless you're going to be tapping
on the shelf, you won't suffer from the same sound issue as that. So where does that plug
into? Did you say it goes into a mixer? Yeah, it goes into a mixer, which is then connected
via USB to my studio PC. So it's essentially the same as the setup you've got, but I've
got a mixer between the microphone and the PC. The reason we've done it that way is because
in our setup, we have two microphones, and having two USB devices as sound devices can
be problematic when you're doing setups like this. But also, we still have yet to get
around to re-recording the Ogcamp chat that we did as a family after we went to Ogcamp
in August, because the way I recorded it was abysmal, I wasn't going to let that get published,
but I have actually got a handful of more microphones in a box down here, which I can plug into
the mixer, so we can have four microphones live at a time. Okay. So that means you can
kind of just levels on that without doing it in the computer, then I'm getting with the
slidey things. Yeah, you can. You can do that. You've got tone controls as well, so you
can adjust the top-end and base and mid-range also. But this one also has an additional
functionality, is that there is an inline compressor on the channel itself. Okay. So what is a compressor,
because I've heard about it before, people are talking about it. So I can't give you a technical
definition of it, because I'm not a professional audio engineer, but in essence, what it does is it
makes an attempt at bringing up the quieter bits and bringing down the louder bits, so that when
you're speaking normally, you're at a certain level. If I start to speak really loudly, in theory,
you're still at the same level, you just sound like you're louder, but you're not. But if you start
talking really quietly, you can still hear me very clearly because of how the compressor is working.
So it's working in the background to try and make it. So the variance of natural speech, where
sometimes you'll be louder and sometimes you'll be quieter, tries to retain pretty much the same
level across all different ranges. That's a non-technical description. Okay. And so there is a
couple of different kinds of mics, isn't there as well, with the record. I can remember one is
the record directly. You have to be sitting in front of it like our mics, and if you move away
from your face away from the mic, it doesn't kind of pick up so much where you can get. And the other
kind of mic, where it picks up like a free 60, like one of those snowballs.
Yeah, so that's to do with pickup patterns. So the pickup pattern that we've got on our microphones
is called cardioid, which essentially means that it's only going to pick up noises that occur
directly in front of it, or slightly from side to side. The other one, which I think the
blue Yeti has an option for, is omnidirectional, which means it can pick up sounds from all around
it, which is not necessarily conducive to a recording situation such as this, because when you're
talking just you into a microphone, or in the setup we've got here, two people talking to
two microphones, you don't want to pick up sound from anywhere else other than directly in front of
it. Yes, if not, if you have like a one get everywhere, you can obviously, if you've got a PC or
something in the background, that can pick up, and if you've got to cough, you can kind of come
away from the mic to, so you don't, so you don't, so you doesn't get picked up. Well, we won't get
picked up as loudly, sir. Yes. I mean, if I, if I, I'm directly in front of my microphone at the
moment, if I kind of go directly 90 degrees to one side of it, I'm now talking directly at the
microphone, but I'm talking at the right angle to it, it really drops the sound out quite
significantly. So what's the best, I guess the best area to record a podcast is in a nice,
quiet area. It is, but I recognize that in a lot of situations, that's not always possible.
Now, you mentioned about the different pickup types of microphones. There are different
like capsule technologies of microphones as well. The kind of microphone we've got is a dynamic mic.
And dynamic mics are much better suited to areas that may be prone to background noise.
So I'm really fortunate here that where I am at the moment, we call it the studio, but it's
just like in the station to the kitchen. There is a very little background noise. In fact,
the only noise I can hear at the moment is coming from the studio PC, but it's such a small fan,
you can't hear it anyway. But if the heating was on, I can see the boiler from here. Then
you wouldn't necessarily hear it through the microphone unless I was speaking.
If that makes sense, because I've got a gate on what you're listening to, I've got a gate,
but on this recording, I don't. So you wouldn't necessarily know that. But with a condenser microphone,
which is the other kind, condenser mics are usually used by professionals in professional
environments where they are operating in soundbooths, because condenser mics do have a much
better audio quality, in my opinion. However, if a net farts in the corner of the room,
the condenser microphone will pick it up. They're so sensitive to noise around them.
Before we got these Samsung mics, we did have a pair of condenser mics,
a Berenger C1s, I think they were. And the sound quality was really, really good.
But if there was any background noise whatsoever, it picked it up, and that can't be problematic.
So the next thing is about, we could only record an audacity. To my term, it's what levels
is what gets beat. And I think, unless you're like a really good audio kind of person,
you know, for non audio person, I think that's quite hard to understand. I don't think you've got
like a musician's air kind of thing. It is knowing that kind of level kind of thing, because I know
that when you talk, you can see that it was telling you earlier about the waveform going between
0.5 and might as well. But there's another thing where you can see where it starts,
like minus 57, all the way to like zero kind of thing. And then that you talk, it does peak out.
Absolutely. So in audacity, and I would imagine a lot of other audio workstation packages as well,
when you're recording, you usually get some kind of volume meter. The one that I've got on my screen,
which is that the very top of the screen in audacity is measuring the sound that is being recorded
directly from what's coming in through the mixer. And as you say, it's a scale that runs from minus 57
to zero. And so that's a scale where zero is the absolute maximum that you should be recording at.
If you go above zero, what will happen? It says a little red mark on the right hand side,
just to the right of zero, which will stay on. To say, at some point recently, you have peaked and
gone over that level. Now, when you're talking normally, you'll see that up to a point that
bar is green. And that's where you want your level to be. Under normal circumstances,
keep your volume, your recording level, into the green. If you go into the yellow, a couple of
times, doesn't really matter. But if you find yourself staying in yellow, creeping up to orange,
maybe even going into red, then your either level is too high or you're too close to the microphone.
Now, when you're doing a recording at any particular time, it's not always as easy to, if you've
got a mixer to adjust the level, or if you're using something like audacity to have to move your
mouse across to the little slider on the right hand side and take your microphone level down a bit.
The easiest way to get around that is just to back up from the microphone to smidge,
until you're back into level. That's handy to know then, because I have peaked a couple of times.
But that's what I think is always good to have that audacity on the screen for me anyway,
to kind of see when I do peaked, so that it's a visual reminder. And then,
getting to that matter, if you've got different people, if you've got multiple people recording,
you want to keep everyone the same level, or that better be done in post-production.
If you're in different locations, then it's next to impossible to ensure that everybody is
keeping to the right level. It's kind of down to the individual to make sure that they're staying
at the level that is as optimal as possible. In order to level off when you're in post-production,
so after everything's been recording, you've got your distinct audio files. And this is a very
good argument for each individual to record their own audio locally, rather than having everybody
recorded in the same audio file, because you then have that ability to level off the individual
channels to remove things I coughed earlier. I'll edit that out, so you won't have heard it.
Things like that, but you can't do that easily if you have all the audio merged together in one
file. So if both you and I were speaking at the same time, there's no way to identify in the file
itself who is speaking at any particular time, and you can't isolate one individual out of that.
If you're recording your own file locally, then by combining them after the fact as separate
tracks in a package like Audacity, as long as you've lined them up, then it's then easy to isolate
individual items like if I drop my keyboard or click the mouse or something like that,
to be able to remove those. Once you've obviously recorded it, I mean, you know what we want us to
could say the files are fact-file with the guessing is a no-no-lossless format.
Yeah, so Flack is the, it is, it's classified as a lossless format because it is compressed,
but when you uncompress it, it is back in the original form. Audio formats like MP3,
Og, M4A, I think is the one that the Apple seems to prefer. Their classified is lossy,
which means that they do get compressed to make the file smaller with no appreciable loss of quality.
But when you uncompress them, you have lost some definition, you will lose some of the data within
there. A classic way to demonstrate this is to go through a cyclical process of recording a piece
of audio, save it as an MP3, close it, open it up in Audacity, as an MP3, and then export it as an
MP3, and do that a few times. And what you'll discover is that the file you get out of the end of
it after three or four attempts of doing this is going to sound really quite bad because you're
compressing a compressed audio file again and again and again. When you do it through a format
like Flack or ALAC, which is the Apple equivalent, because it decompresses back to its original form,
you don't get that loss of fidelity. So it'll always be as good as it started off.
So when you're doing post-production, so like with the admin podcast, I always ask you guys to send
me the files in Flack format because I'm going to be loading them in and making changes to them
and then saving them again. If you sent me the files in MP3 format, I'd be taking an already
reduced quality file in the form of NMP3, making changes to it and then saving it as an MP3 again,
which means you'll be compressing an already compressed and uncompressed file.
Which is to be honest, that's how I started podcasting. I started podcasting by recording us
in MP3 and then editing it, saving as an MP3. And you can tell the difference. It's slight,
but you can tell the difference. Something I did mention before we would go through kind of on the
post-production stage about audio monitoring and like listening to yourself while you're talking,
kind of thing, because I never really got on with that. I think when I first got my mic,
I used it once, but then it never. But that's clear, I haven't managed to get it working, but I think
on the mics we've got, you can plug your headphones directly in the back of the mic.
But I haven't managed to get it working. How have you got, do you do that and how
and how have you got that set up and what's the benefit of doing it? I'm just going to try something,
give me a second, because I'm just now unplugged my headphones and I'm coming to
plug my headphones in the back of my microphone and I can't hear myself.
And I think that's because the self-monitoring of the two QU, Q2U microphone even only works in USB
mode. So no, that doesn't work for me. It certainly should work for you because when you plug your
microphone into the USB port, all the audio input and output should be redirected to the microphone.
So the headphone socket on your microphone then becomes your speakers. Yes. So anything from your
computer, from your laptop, any audio played from that should then come through your earphones.
That's how it should work. Going back to the original question about monitoring,
yeah, it can be incredibly disconcerting to listen to your own voices you're talking.
The way we've got our audio set up here is there is a minute delay between me talking and then
me hearing myself. She's also incredibly off-putting. But after a while you get used to it,
if you're only recording for your own benefit and you don't want to hear yourself,
then you don't have to wear headphones at all. If you're just talking to yourself,
talking to yourself, if you're just recording yourself and no one else, then if you don't want
to hear yourself, take your headphones out because that way you'll hear yourself better through your
own ears. But if you're talking to other people like we are now, by having your own voice coming
back to you, you're hearing yourself at the same level as you're hearing the person you're talking to.
One thing I have noticed is if you are having a conversation with somebody, let's say on the phone,
you're on a phone call, you've got your earbuds in or your headphone or headset on and you're on
the phone to somebody. You'll find that if you're talking, sorry, if they're talking and you want to
interrupt them, you start to shout because you can't hear yourself over them because they're in
your ears and you're not. So the idea of having you being able to hear yourself while you're recording,
as I said, as off-putting as that may be, it does actually make for a much more natural interaction
between the person you're talking to because both you and they are at the same level. Does that make
sense? Yes, it does. So moving on to the post-production, so once you've got your file,
what kind of things do you do to make it sound better or what's your work flow?
So I'm going to have to use Admin Admin as the reference point here because I don't do any post-production
editing on the bug cast. What gets recorded is what gets released as a podcast, which can be
interesting. It sounds okay to me. But I haven't got the good airs days. Yes, sometimes I do wish I
did edit it, but because we record it and stream it live as we go, it seems pointless editing out
for the podcast when the people that listen live got it as it came out. I suppose it kind of
reigns us in a little bit. If we know that we're not going to be editing stuff out, we tend to be a
little bit more restrained, the kind of things we can potentially get away with. Anyway, back to the
point. On Admin Admin, it's part of the process of post-production, which I will admit does take a
fairly reasonable amount of time to do. I go through, well, once I've aligned the tracks,
so I get your audio, I get Jerry's audio, I get John's audio. I know then align them so that
when you play them back, you're playing them back in the right chronological sequence. Now,
the trick that you guys employ is you say a word at the same time. So that could be biscuits,
it could be, I think you did Ansible once. All I've got to do is find that word in the recording
and align the three tracks up so that you're all saying that at the same time.
And then, as long as I don't move them, then that should be the correct alignment for the tracks
for the entirety of the show. So the next thing to do once you've done that is start to go through.
And what I do is I actually start to play it. I play the show from that point and look for
things that possibly shouldn't be there. So mouse clicks, coughs, burps. Yes, I have edited out a
number of burps in the past. I said mouse clicks, breathing down the microphone is a common thing,
but it's not a problem because it gets edited out anyway. Sometimes the recording,
sorry, sometimes the flow of the show will stop because
something needs to go and find something like I did earlier on, but again, that'll get it to
out so the listens won't hear it. That I try to remember the name of a podcast,
forgot it. So I went off and opened my phone and found it and then carried on, but that'll get
removed from the from the final sequence because of how it's recorded. So going through the
entirety of that, it is a time consuming process. If you've got a podcast that is, let's say, an hour
long, 60 minutes, I think it's fair to say that it would probably take three, maybe more than three
hours to go through it to remove all of those artifacts, but it's, in my opinion, it's worth it
because of how it improves the experience of listening to the show in the first place.
Because if you've got somebody who is clicking a mouse, sorry, owl, but you are clicking
your mouse at the moment, doesn't bother me in the slightest. The listeners won't hear it because
I'll edit it out, but it's a natural thing to do. You're going through. We've both got documents
in front of us. We're going to be scrolling up and down them and typing in them. That kind of
stuff can be removed because the listener doesn't need to know that. It's almost as though you're
creating an illusion of how things should have been as opposed to what happened in reality.
And another thing, although we're recording ourselves locally, we're actually communicating
in real time over mumble. When I first started producing, we took the recorded files that came
out of mumble rather than locally recorded audacity. There was a serious problem with the way that
mumble recorded it is that if you spoke too loudly and it clipped, it actually sounded distorted.
It was a really horrible sound. I did my best on those recordings to try and go through and remove
as much of that crackly sound as possible without removing the word that was being said at the time,
which is a forensic process. You have to zoom right into the waveform and find where these
distortions are. They're fairly easy to spot, so it's not that big of a deal. But as you're going
through an hour's worth of audio, it does take a while to pull those artifacts out. Fortunately,
because we're now using the audacity recordings, we don't get those artifacts, so it's helped speed
things up a bit. But they're all things that you need to consider when you're editing a podcast,
or even when you're recording a podcast, to try and reduce the amount of post-processing that you
need. That's why we change from using the mumble recorded files to the audacity ones, just to make
the audio cleaner. Yeah, and trying to be better at not clicking the mouse and make your job a lot
easier. Well, that's not really your problem to be fair, but I suppose it depends very much on
the generosity of your producer. So you say you remove it, do you mean just like silencing it,
or I'm guessing you must, if someone's already speaking, someone's clicking the mouse on the person
who's not speaking, I guess you just silence that audio out. Absolutely. So if you've got your
three tracks in front of you, the way I lay the tracks out for you three, yours is on the top,
then Jerry's then John's alphabetical, you understand. So if Johnny's speaking, which is invariably
the case, and you're clicking your mouse, then it's very easy. All I have to do is just highlight
the section of audio where those mouse clicks appear. Just that one track, where the audio is
that you want to remove. And then if you go into the edit menu, remove special and then silence audio,
it effectively just silences that particular highlighted section of audio. You can't delete it
because that will put everything after it out of alignment. That's why I was guessing. Yeah, so
you would just, all it all it does is it just that piece of highlighted audio just gets put down
to zero. It's effectively as though it never existed in the first place, but it's still there
in time. You've still got those two or three seconds of audio there, but it's silent.
So once that bit's done, so you've deleted all the bits, the cost and everything at what's the
next kind of stage to do. Okay, so once it's been edited down, so the three or how many
other channels you've got are clean and you've topped and tailed it so that the stuff that came
before it and the stuff that's come after it aren't there anymore. You've effectively got
your final mix of audio. Okay, now you may have music to put in, you may have an introduction
to put in, but that is your core audio done and dusted. So I suppose it depends on what you
want to do next. The, the process I go through is I go through and use a service called
Authonic that I did hint to earlier on in the conversation. What that does, I send all three
audio files through Authonic and what it does is it levels them for me, but it levels them using
using loudness rather than the amplitude, the actual waveform. So when you get your,
your tracks back, it's actually merged them together into one track. It's called a multi-track
production. You put your three tracks in, you specify you want them leveled, and it then
merges them into one file that you get back with all three tracks merged into one. So at that point,
you're kind of at the stage now where you can't do any more cleansing of that file because
it's all being merged together. Once that file comes out to be fair, that file is good enough to
publish in my opinion. But if you then have further stuff to do like, if you have an introduction
that's recorded separately with the Abin Abin stuff, because the introduction is recorded in
the exactly the same way, I do exactly the same thing to it. Even though it's only, you know,
20, 30 seconds long, three files, clean them, top and tail them, export them, stick them into
Authonic, multi-track, level them one file out. So I end up with one short file that's an introduction,
one longer file that's the actual show content, and then I just put tunes around them to
finish off, and then that becomes the final episode that I then send to you for publishing on the
site. So you put the music in afterwards or the intro and outro after it's been through that
service? Yes, in the case of Abin Abin, yes, the music is the last thing to go in.
And how does that work with the levels and stuff, kind of thinking in my head? Did you do that
in Audacity or something else? No, it's done in Audacity. So the two audio files from recording,
so the intro and the main section, I put those in and put them roughly in the place that they need
to be. So the intro goes in at about 10 or 15 seconds in from the audio, and then the main part
goes in about 5 or 10 seconds after the introduction is finished. I'm going to take a series of snippets
that I've created from the theme tune, and the theme tune is looped. So there's one part that is
just the, it's very typical to explain this. So there's one part that is just the very first part
of the theme tune. Then there's another part in the middle that gets repeated for as long as the
intro is happening, and then there's another part where the intro theme tune fades out. So if the
intro is five seconds long, which typically isn't, or the intro is a minute and a half long,
which it typically isn't. By having that repeating section in the middle, it doesn't matter,
because all what happens is you'll get that initial start of the theme tune, and then a repeating
portion, and then a fade-out at the end. And the, all the listener hears is some music in the
background, but it doesn't feel out of place because the, the various segments that are used to
make up, however long the intro is going to be, they're designed to be placed together.
So if you take your first bit, and then three or four middle bits, and then your fade-out bit
at the end, it sounds like it works because of how they've been, how they've been created,
how they've been sized, and exactly the same happens with the outro as well.
Yeah, and then you can detect, port it as an MP3, and then you, then I can publish it.
Yeah, and that's exactly what happens. Okay, cool. Yeah, I like the idea of that,
the, it was called an authentic, orphonic, or, orphonic, yeah, orphonic, okay.
We've put a link in the show notes for that. Yeah, definitely. Anything else?
You covered most of my questions, what I've, what I've asked about, what all the different,
what the project production is, and they're getting the recording audio. I'm only
finding to do my, the 30 days of November, is that right? 30 November? I'm guessing it's 20 days.
Yeah, yeah. Yeah, if I'm doing it just on myself, I'm guessing, just using that,
it's going to be a lot more easy than trying to edit multiple tracks into one kind of thing.
So are you paying to do as well then, Dave? I am, yes. It's not my first attempt at Naftopomo.
I think this is either my 4.45th attempt. I say attempt, you know, I have completed it,
but I didn't do it last year, but I did do it in 2016. But no, I just fancy doing it again
for the chuckles, and I'm going to try and, I think, last time I did it in 2016, I made the mistake
of not planning it in advance, not having a purpose for doing it. And as a result,
I, I dried up fairly early. It was about day seven or day eight, where I actually ran out of ideas
and thought, do you know what? Forget this. I'm just going to play music. I ended up talking
for about two minutes and then playing a song at the end of it, which wasn't where I wanted it to go.
So this year, I'm putting in some, some planning time. And one of the things I want to do
is at least once a week, I want to get on somebody else who is doing Naftopomo as well.
So in addition to you, there's at least two other people that I know, two other broadcasters who
are going to be doing Naftopomo this year. And I figured it might be fun to do crossover episodes
where let's say you and I record an episode together, and we release the same episode on both
of our Naftopomo feeds. Sounds like a plan. But I'll put a link in the show notes to my feed,
and I guess we'll put a link to your feed as well so that we've got them. So this is not the
where it's going yet, but yes. Yeah, I haven't decided yet either. That's why I just want to
revamp my website at the other day too. From the 9th or the 2000s, when I had it with a
just normal HTML site, now I bring it into a WordPress site. So yeah, okay. Thank you very much
for your time, David. It's been fun. I would like to end by saying when it comes to doing podcasting,
it really isn't rocket science. Just because I've got to set up that has multiple microphones
and a mixing desk and a dedicated studio PC and a dedicated stupid studio era,
I've been podcasting for over 10 years. So because podcasting is such a big part of who I am and
who we are as a family here, it's only natural that we've actually managed to build up this
kind of equipment over time. But I'll be perfectly honest, you can get a pretty decent sound quality
out of a mobile phone. Just recording into the microphone of them on the mobile phone.
There are very inexpensive, and I'm talking like $1.52 perhaps. What are called lapel microphones
or lavalier microphones. You can buy off of Amazon, which you can attach, like a tie clip,
the kind of ones they use on telly. You put it on your collar or something like that. Plug
that into your mobile phone and just record from that. And the quality is really, really good,
but it also means, A, you don't have to hold your phone. And B, you get a consistent sound out of
it because the microphone doesn't move. But you don't have to be a professional. You don't have to
have, you know, spend loads of money on equipment. You don't have to spend money on websites.
There are plenty of web services that will host your audio for podcasting. Anchor is one.
Audio boom is another one, but I don't know whether they're actually struggling at the moment.
I heard rumors that audio boom might be either being bought or sold or something
in the not-too-distant future. But there are plenty of places you can record and host your audio
for free. A hacker public radio, of course, being one of them.
Yes. Yeah, thanks again, Dave. I'm guessing we'll catch us on another show sometime.
Well, you haven't plugged your podcast yet. Oh, yeah, I guess so.
So I host a podcast called The App and App and Podcast, which I have done a couple of episodes
on HPR before. I think I've done two. One was coming from our first odd camp and then
talking about what we and how we were talking about our experience. I don't know if I've
done a second one, but it's like 2014, the last time I recorded. And I heard there's a
call for shows. So that's why I wanted to kind of record this show. Yeah, but App and Podcasts
is kind of a techie podcast for cis-admin DevOps people. It's not just Linux. It's always
not just Linux, Linux, Windows, VMware, virtualization. It's just where we don't really have a,
we have a kind of a rough idea what we're going to talk about, but we've and normally end up a lot
talking off topic. We have three guys just chatting about what would have been up to or anything
we've had out, really. Stream of consciousness was one of the descriptions that you were given
over email recently. Yeah. Which isn't a bad thing, I'll be honest. So yeah, and then Dave does the
the pre-production of it. I think it must be the last 10 episodes now, I'm guessing.
Something like that, yeah. And it's definitely improved. And this is a can. It's going up, and I mean,
do you want to guess you want to just plug your cart, your podcast? It could be brewed not too,
wouldn't it? So I do a podcast with my good lady wife on a weekly basis called The Bugcast,
and that is a music and chat show, which doesn't really have a format or an agenda as such.
She's just some damn good music and a bit of waffle in between. Yeah, I do like, I definitely like
the chat. It's quite good to chat in here, and I know it's easier to listen to it than we
kind of have to release it doing the hash work when the hash thing. Excellent. But we'll put the link
to both of those shows into the show notes. So if you want to have a listen to those,
you'll be very welcome. Yes. Okay. I guess we'll see everyone soon. Ta-da!
You've been listening to Hacker Public Radio at HackerPublicRadio.org.
We are a community podcast network that releases shows every weekday, Monday through Friday.
Today's show, like all our shows, was contributed by an HBR listener like yourself.
If you ever thought of recording a podcast, then click on our contribute link to find out how
easy it really is. Hacker Public Radio was founded by the Digital Dove Pound and the
Infonomicon Computer Club, and it's part of the binary revolution at binrev.com.
If you have comments on today's show, please email the host directly, leave a comment on the website
or record a follow-up episode yourself. Unless otherwise stated, today's show is released on the
creative comments, attribution, share a light, 3.0 license.
