Episode: 3069
Title: HPR3069: Linux Inlaws S01E05 Porn and Skynet
Source: https://hub.hackerpublicradio.org/ccdn.php?filename=/eps/hpr3069/hpr3069.mp3
Transcribed: 2025-10-24 16:13:15

---

This is Hacker Public Radio Episode 3,069 for Thursday 7 May 2020.
Today's show is entitled Linux in-laws Season 1 Episode 5 Born and Skynet
and is part of the series Linux in-laws. It is hosted by Monochromec
and is about 67 minutes long
and carries an explicit flag. The summary is
This is Linux in-laws, a series on free and open source software.
BlackHumour, the Revolution.
This episode of HPR is brought to you by AnanasThost.com
Get 15% discount on all shared hosting with the offer code
HPR15, that's HPR15.
Better web hosting that's honest and fair at AnanasThost.com
.
.
.
.
.
.
This is Linux in-laws, a podcast on topics around free and open source software,
any associated contraband, communism, the Revolution in general,
and whatever else, fans is critical.
Please note that this and other episodes may contain strong language,
offensive humor, and other certainly not politically correct language.
You have been warned. Our parents insisted on this disclaimer.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
So, let's continue, but first and foremost, let's see what's in store for this episode.
I'm going to shed a little bit light on the news, what has been happening since we last posted an episode.
Yes, it's been a while. We tried to record this every fortnight, but then life got in between.
So, I'm on my own, and it's been I think a month or something like that since we last posted the last episode.
So, let's see. Okay, first section news, second section.
And this is probably of interest for most of your listeners.
We're going to shed some light on audio and video production with free and open source software.
Of course, the idea being now that we are all working from home due to the wonderful pandemic called Corona,
or something, whatever, high-necking cards back in the rest of them are making any of this, but maybe not anyway, it doesn't matter.
So, with that ample amount of time on our hands, this is a, what we're going to do with it. Of course, you can read books, you can get around to watching this,
or almost all forgotten series on Netflix, but once you've watched them all, you are really probably wondering what to do with your time.
So, why not take this dream, this long anticipated vision of doing your own adult content websites and putting this out there,
showing porn hub, you porn, X videos, and whatever, friends keep telling me, is in the name of the websites to check out.
Needless to say, I haven't done this yet to show them their reigns.
And for that, basically, of course, you need some, well, basics, let's put it this way.
So, this is a two-part thing. The first part will be shedding more light on the theory behind this,
as in what is really video, how to stream this, how to modify, how to edit content, and afterwards, in the second part of this overview, let's put it this way,
we're going to shed more light on tools associated with that fear. Needless to say, it all comes in handy, whether you want to do a live streaming porn website,
or whether you want to do this kind of with clips, that is pre-recorded content showing this on the website.
But first, let's start with the news. So, what's been happening since we last posted an episode?
It's now the 23rd of April, I'm just recording this. Of course, the big news, focal fuzz has, and Ubuntu 20.4 has just hit the streets.
This, of course, being a long-term support release, canonical-put-a-series amount of effort are wrecking into this.
So, the novelty, according to the change-lock, is a revamped GNOME software shop that also supports snaps apart from porn updates and debit packages.
And why is that important?
Some updates for those of you who have a laptop or use a machine that the open firmware consortium, I think, is the name of the organization, supports you can get your firmware updates even for the Linux machines, sorry, for running Linux rather, through something called a firmware update manager.
This is normally a packet as part of your ordinary little distribution. Ubuntu has it in its reports, so you just have to install it.
And the beauty is, it's a command line tool allowing you to update the firmware on the fly.
So, the way it works, it has a demon part, it has a command line front end.
And you simply say, check out the latest version, I think it's called firmware update, a FWUPD refresh or update.
And then it goes out there, see if there is a new firmware version and then it downloads it and it stores it transparently on your PC.
That, of course, includes a reboot of sorts, because essentially it works, it downloads a package, puts this package somewhere where the buyers can find it, reboots.
And the buyers is intelligently enough to see that there's a firmware update pending, takes the firmware, flashes the chipset, and then it basically reboots into your Linux or into your operating system of choice.
The beauty about this, you don't need to resort to legacy software like Windows, because this firmware update runs on Linux.
So, no need to set up a stick containing Windows, booting into that stick, then updating the firmware and then booting back into Linux again.
So, this is a kind of really nice shortcut, but for that you have to have a laptop or a generator machine that is supported by this firmware consortium.
Probably your hardware, your hardware window of choice on his or her website that will tell you to how to go about this.
What else is in focal fossa? By the way, the name comes apparently if I'm not completely mistaken, comes from a Madagascarian cat of sorts, something that's a Wikipedia article that explains it all.
Okay, the next thing that is new is that the GNOME software shop also supports snaps.
Now, for those of you who are familiar with Ubuntu asin have been around long enough, you know that since I think 16 or 4 asin by Yonic, snaps are the big thing in terms of pre-patch software running in their own almost container format.
There are three of them, there are flat packs, there are snaps and there are also something called app images.
The idea of course is similar to containers to give you a format that you can simply download and then install this without having to install all the dependencies.
Similar to containers, one of the main challenges with pre-packaging software is normally you are relying on version ABC of a particular library, it has to be database version X, Y, Z and all the rest of it.
So that sort of thing, especially if you're relying on particular features that come with a specific version, normally pre-packaging software that cases nightmare.
And hence this kind of raving success of containers.
Yes, containers can give you a nice layer of virtualization, are cheaper and faster to spin up than virtual machines, but at the end of the day one, the main advantage when using containers is actually that you can build your own user land with all the baggage that the full blown VM installation brings with it.
And snaps and flat packs are similar in that range, although they are again much more lightweight than your ordinary container.
So the idea is to just package similar to containers, to just package the dependencies that you need into a small, I'm also to say file system because it's essentially very similar to something called an image that likes of Docker and so forth.
And friends bring with with them. So once you have this kind of basic user land, you simply compress it and then it snaps flat packs and so forth.
The main differences between flat packs and snaps and an image on the other side is that snaps and flat pack as formats all have their own repository.
In terms of there's a flat pack repository, there's a snap repository and that is actually called snap craft and it's maintained and it's maintained pack on a call that allow you to simply search for a particular flat pack or snap then download it and then you install it.
And this is supported in both cases flat packs as well as snaps by an ecosystem. So you have your demon running the background.
You have your command line front and that will take care of searching, installing and removing a flat pack or snaps and all the rest of it in contrast to this app image.
You think of app images more like self-contained executables because the way it works is you download an app image, you make it executable, you execute it.
And then the app image will take care of well executing the binary that is in it.
The main difference is that an app image doesn't have the ecosystem that snaps as well as flat packs bring with them.
Okay, enough about this. What is also new with a 20 or 4? Again, it's an LTS.
It will contain kernel version 5.4.something, check out changelog.ubuntu.com for the final technical details.
That is of course in a long term support kernel in itself that is I think valid until 2021 whereas the Ubuntu LTS is I think valid at least five years.
So there will be certainly a kernel update as part of the LTS but normally canonical has the trade or does actually back parts of new kernels too to old LTS especially if you're talking about long term support release.
Okay, and maybe also the big news is that 20 or 4 will be shortly available for something called the WSL.
Now those are few who still use Windows, you probably have heard about the Windows subsystem for Linux which essentially is a with WSL 2.0, a full-blown Linux kernel executing the user land of your choice.
Originally it was just Ubuntu but of course you can get user lands running on WSL for open suze for Debian for CentOS for Fedora.
You name it, chances are even arch. Changes are that there's user land running on WSL.
The main difference between WSL and W1.0 and 2.0 is essentially that 1.0 was just a mapping layer from a Linux based user land to the Windows anti kernel which has all sorts of fun implications.
For example, you could not run a Windows system natively because essentially what you had to do is you had to use an X Windows server running on, for example, SikWin and then you had to tell the WSL 1.0 user land that there is actually an X server running outside its user land and how to connect to it.
WSL 2.0 would have all that integrated so because you have a full-blown Linux kernel at your disposal with the corresponding compatible user land, you simply use it as it would be a full-blown Linux system without these kind of wargrounds.
Let's put it this way. Again, to me, this is another move from Microsoft to get on developers' desktops essentially because normally these developers would use, if they would be programming on the server side, they would be using either Macs running OS X or actually Linux or maybe even Windows with a hypervisor like Open VirtualBox or some flotters installed to be able to use.
With WSL 2.0, they have a full-blown Linux user land at their disposal without the need for any additional software.
So again, that shows, I think, Microsoft's commitment to some extent to converge these, especially from a developer's perspective, to converge these kind of, I wouldn't say, universes but certainly ecosystems.
For those of you looking for the exact point in time on when this release will become available, there's of course the canonical website.
But there's also a tool called do release upgrade, which is part of the overall system administration tool set.
And this tool allows you simply to automate the whole upgrade process, giving you the fact that you have an existing installation already.
So if you're on an LTS and the last one is 1804, simply go onto your command line and say do release upgrade.
And then you have also the possibility of say minus D, which is the development version, which you don't probably want to use if you are running a long-term support release.
And then the whole thing will be upgraded on the fly from the command line.
But saving you this, you can go to a website called change logs dot ubuntu dot com.
And that website contains then all the release information for the LTS is for the development releases and the normal ones.
And if you see a file called meta release that has at the very bottom a focal fossa entry, that is the point in time when then to execute do release upgrade.
You'll find the URL of that website actually in the show notes.
So for those of you who want to save themselves off checking every day or something like this, whether the new releases are available, simply check out that URL and then you're good to go.
What else has been happening in the in the world of open source currently 5.6 has been just officially released the main probably two big things is the native support for wildcard.
For those of you who have been living under a rock for the last three to four years.
Wirecard is the better open VPN. Open VPN of course is a free and opposite software that allow that implements a VPN connection.
And the rumor has it it's quite complicated to set up being off from personal experience I can vouch for that, especially if you're looking at more complex scenarios like connecting to a VPN gateway and then going into an internal company network VPN can be tricky to set up.
Wirecard and in contrast just requires a couple of keys and a very straightforward configuration and then you are off to the races.
I have been seeing people using Wirecard after a five minute configuration on the command line. Wirecard requires some sort of crypto functionality and in contrast to open VPN who mostly relies on user land implementation like open SSL and so forth.
Wirecard actually supplied code for kernel integration. The pull request goes back as I think back to 16 or 17.
While the Wirecard developers contributed code to the kernel Linus finally accepted it think about half a year ago and it was destined for the inclusion 5.6 and that's exactly what we're seeing now.
So Wirecard now can use crypto mechanisms running on the kernel at full speed rather than having to rely on some sort of third party library or external user land integration.
The beauty is of course why you can also get Wirecard for OS X and Android and the developers are working on a non Linux version with the kernel functionality implemented in the user land.
So you will be able to also use this on BSD like systems like OS X and I think there's also a window space port in the works.
Certainly it's running on all Linux sort on all Linux systems because now with the expression with kernel 5.6 it's baked into the kernel and other systems are just available to just go to the
to the corresponding website you find the Wirecard website actually in the show notes.
What else? Yeah, Python 2. Interesting thing there because as probably most of you know, Python 2 has been deprecated last year.
There was a famous Python enhancement proposal by the name of PEP 404.
Yes, number is a little bit of a giveaway detailing plants about Python 2.8 which of course will never happen because as the PEP outline you find the link to the
PEP in the show notes that PEP said clearly Python 3 is the way forward and that's exactly what Giro said about I think 10 years ago or something like this.
And the end of life was for Python 2 was put into existence with that PEP around the end of 2019 which was last year.
So quite a few distributions have decided to prolong this agony. Let's put it this way.
And at least provide security backpots for the time being the project itself has just issued the very last version, the very last release with version 2.7.18.
I'm going to put a link into the show notes about a mailing this entry detailing this and the I think the C Python maintainer is quite relieved now that the days of Python 2 are finally over.
Needless to say, for any of you who are using Python by now you should have switched over to Python 3.
I can only recall one specific project that is still relying on Python 2 of course that is Moin Moin.
The famous content management system running on Python 2 and if Laura say anything to go by I read the web page because the luck that I'm supporting the local Linux user group is using Moin Moin version 1.7.
I think that is still relying on Python 2. The website spoke about Python 3 support having this in place for Moin Moin 2.0 which is I think still in better.
But having this at the end of last year apparently that didn't happen. So the last time I looked at the website they were kind of still closing the gaps in terms of relying on some third party Python modules
that hadn't been ported to Python 3 yet but I think we at that station I was I think it was about two or three months ago.
We were only looking at two or three remaining modules.
That's a show I was I was almost tempted to say let's give that a shot myself and simply port these to three remain models to Python 3 so that they can close that gap and move on with the development of Moin Moin 2.
Because at the end of the day we are not the only organization using Moin Moin.
It's still a widely used contact management system especially in the open source software world.
But now on to our main topic how to set up a website for the distribution of life or recorded content.
And of course that doesn't just apply to adult oriented content. Whatever your hard desire is you can simply use that as a blueprint to do movies or to do content with little fluffy killer bunnies with your favorites outlaw content.
Maybe you want to distribute lovely recordings of how you prepare the 15th type of legend of lasagna as in doing a cooking show maybe your significant other has certain ideas of doing something else than just putting her makeup or other fashion oriented content onto YouTube.
Because YouTube is great but having your own website distribution content is a whole different ballgame.
So let's get into the details.
I'm going to focus here on how to do this from a technical perspective assuming that the relevant kid is already in place.
So make up your mind about what camera to buy, what computer to buy. Of course Linux based systems do have their choice especially when it comes down to the camera.
You can get some beautiful cameras that run Linux out of the box.
What is important though when you buy a camera and this is just in the side because as I said we won't be focusing on the hardware support.
On the hardware kit basically that you need I will be focusing on the software side of things just as an aside.
When choosing a camera just make sure that this camera can either provide you with a raw video stream or already deliver something comparable to MPEG4 or some other compressed video stream.
The big choice is of course to make up your mind whether you want to go for live streaming versus clips.
That looks like a significant difference but at the end of the day it's almost not.
Because if you take a close look live video streams means taking content from a video source and streaming that content live to your viewers.
Internally this relies still on something called a container format which is exactly the same as for clip based content.
So at the end of the day live is just an extension of recorded content.
Taking a very close look.
Okay let's cover live streaming first before we go into the details of how clips internally work or generally record content.
Live streaming essentially relies on two protocols.
Both of them would be support to some extent by modern browsers. One of them is called RTMP.
Real-time messaging protocol and the other one is called WebRTC. Web real-time communications.
Both of them have the central idea of taking contents as a live contents from a source like a camera.
Grabbing them and forwarding them via HTTP or a similar protocol to a browser interface.
Both of them have their advantages and disadvantages.
RTMP would be TCP based and because of its age has quite a significant amount of software that supports it.
For example you would have RTMP support built into any browser under some almost.
So the usual suspects like Chrome, firefox, opera, even the Microsoft ones supported.
So it's pretty comprehensive and it's also TCP based.
So these are the kind of big advantages.
It's somewhat older than WebRTC but it's been tried and tested for a long time.
The major disadvantages is for example it doesn't support security in terms of encrypting content right out of the box.
On the other side WebRTC does.
WebRTC is newer, it relies on UDP which comes in handy because you're not paying the overhead that TCP brings with it.
And it's also nifty when it comes down to security features implemented in the protocol.
So for example it supports encryption right out of the box.
So if you're setting up your software ecosystem just make sure that you take a look at both the protocol options in terms of software integration.
We're going to talk about the details in the next part when we cover the surface out of things in terms of tooling.
Moving on to the clips or the rather the container format.
This is important as I already kind of alluded to from two perspectives because essentially on the one side containers essentially represent the current format that you would use when you want to put a clip.
Containing content on the website and yes the container formats are also used to stream video from a source to the browser using RTMP or WebRTC.
So let's take a close look at what a container really is.
A container contains a video data stream an audio data stream and maybe subtitles and maybe metadata like who recorded the content similar to an MP3 metadata stream.
Year of recording genre and all the rest of it.
Let's focus on audio and video formats supported by the most relevant container formats.
Before we do this let's take a look at what is out there in terms of containers in the first place.
For those of you who have ever downloaded a clip from or a video from your favorite provider of choice I'm almost tempted to say.
Or gosh have downloaded a torrent or something like this.
You know that or even taking a look at the camera because the camera is because the camera is producing also a container format.
You'll know that there are quite a few container formats out there.
I'm just going to go through the more popular ones. Let's put it this way.
There's something called MP4 or motion impact version 4.
Another very popular one would be an audio video interleave also known as AVI and last but not least Matroska.
I'm sending this about 15 years ago or more or less and normally featuring the file extension MKV.
So it's either MP4 dot AVI or MKV that you would come across.
The format it would be would be different between the different container formats but they are contained at least one video data stream or most of the time actually one video data stream at least one audio data stream.
Sometimes more than one especially if you're looking at depth version of movies and sometimes also the corresponding subtitles compared to an SRT file if that rings a bell.
At the end of the day the container format takes care of containing these data types and also making sure that the content can be synchronized in terms of you have a video clip and you have an audio clip and making sure that the content synchronized in terms of that the corresponding audio is aligned with the video.
So you don't have a video stream or an audio data stream out of sync.
This is the main purpose of a container format. The corresponding audio and video data streams have timestamps in them that allow us the playback software or the software in charge of doing anything with the container format to synchronize the corresponding content.
So let's go through the video in a little bit more detail. Essentially you have the motion picture expert group, also known as MPAC that has a number of formats out there.
Probably the most popular ones are H.265 and H.264, H.264 the older one and H.265 probably the more recent one.
There are also older formats in that space. If you come across anything that starts with the H and has a dot behind it and a number of digits, you know it's the motion picture expert group.
The idea behind any compressed video stream is essentially you have a picture, then you have a couple of deltas that you need to construct the next picture.
Okay, an example. Say you are recording a video stream, you have a sequence of images. If you would record this without compression, you would be facing a significant amount of data because each and every picture would be recorded at its full dimensions, including the color space, which is a lot of data if you're looking at your typical color space with 8 bits per color.
Times 3, that's 40, sorry, this is 24 bits for each picture. If you're looking at a high resolution movie like 4K, 1080 or something like this, you wouldn't be looking at a couple of megabytes or maybe slightly more than a gigabyte, but rather times 10 times 15 for this.
So the amount of bandwidth that you would need in order to transfer such a data stream is significant. So the idea is to compress the video data stream, the general idea behind all of these video compression formats is pretty much the same.
You start with a C picture, if you will, and then the details do depend on the particular compression algorithm, and then up to the next full picture, you just record the delta as in what change between the different pictures that you basically recorded.
So you actually see this when you take a look at a low resolution impact for a video, especially if you record a rapidly changing scene, you see something called artifacts where the resolution isn't what you would expect, but rather you see the blocks in the picture.
And that's exactly how the compression algorithms do it. They take a look at what's changing between the pictures and just record the differences.
Most of the time that difference then is also compressed with something called a lossy compression algorithm that if the compress doesn't give you the original payload, but rather something very close to it.
You'll see this actually if you take a look, as I said, in the kind of already alluded to any low resolution video on a high resolution monitor, because for example, the boundaries between the different areas of a picture are somewhat fading.
And that's exactly how it works. Same goes for the audio formats. There you have the, before I forget, of course, there's also something called VP9, VP8.
These are content of, sorry, these are video formats originally developed by Google, the same principles, of course, apply.
All of these video compression formats vary depending on their particular traits and characteristics. You find links of the corresponding standards or Wikipedia pages in the show notes.
But at the end of the day, essentially, you're looking at a lossy format that is able to compress video so that it can be distributed across the internet without the requirement of having a terabyte pipe in place,
running at significant speeds. Same applies to audio. We have, we have to differentiate between lossless and lossy compression.
For example, a lossless compression would be flat. The idea behind the lossless compression is essentially you have an audio signal, you compress it, and when you decompress it, when you inflate it, it gives you back the exact quality.
As the original, whereas lossy compression algorithms, like MP3, like AAC, like AUG, basically take the audio stream and compress it in a way that if you inflate it again, you don't get the original back, but something very close to it.
As a matter of fact, if you take a look at something called MP3, it has been around for about, I think, 20 years or something like this. The idea is similar to something called instant noodles.
You take a dish, you freeze it, now it's done differently. You dehydrate it exactly. You dehydrate it, then you have the essence of noodles and vegetables, and you put this into container.
Once you have the container, you simply put boiling water on top of it and wait for a while. MP3 is very much the same way. You take an audio signal and you extract anything from it that the idea is that a human ear won't perceive.
But once you decompress the audio signal, you get something back that is very close to the original, but it's lacking some three-dimensional audio properties that human ears, because of the construction, are very prone to hear.
In terms of, it's very difficult for you to perceive the difference between MP3 encoded, if from properly that is, between MP3 encoded or the stream and the original.
But of course, the advantage is that you get a lot of spare capacity with an MP3 encoded or data stream. Normally you get audio data stream.
Normally you get a factor between 8 to 10, if you do it correctly, but at the expense, of course, of a lossy compression algorithm.
Okay, in the second part of this overview, we will go into these surface other things. We will shed some more light on what software to use for streaming, and also goes without saying.
Detailed software that is normally used for clip production, as in recorded contents, in terms of the transcoder ecosystem, FFM pack hand breaks, and France come to mind, we're going to shed more light on editors that you need to edit content.
And last but not least, we will take a look at the surrounding ecosystems modeling to select blender come to mind.
Okay, and now for the last part of the show, I'm more than happy to present an interview with a Terminator directly coming from SkyNet.
The idea is to give you some insight of what's going to happen in the future and how eventually SkyNet came into existence.
I hope you like it, and let's go down with the show.
I would like to welcome today's guest, a Terminator sent by SkyNet to talk a little bit about the past, the present, the future, and SkyNet itself.
Let her have you on the show to a third. Why don't we start with a little introduction? Who are you and what is your background?
I am a class three Terminators sent by SkyNet 2.0 to eliminate a few misconceptions about the future in general and humanity in particular.
Wow, glad to have you on the show, especially given the fact that there are certain negative perceptions with regards to SkyNet and the survival of the human race in general. Can you shed more light on this?
More than happy too, but maybe a little bit of background will help to understand this.
Sounds good, please go ahead.
Let's start with these time leaked documentaries back in an era you humans call the 80s, you know, inappropriate or parallel known as Legons.
Miscited popular music and hairdos abandons consisting of young male adults which even the archives like to forget about, for a reason.
An earlier prototype of the Terminators you humans know as James Cameron decided to go back in time even before the general approval by the SkyNet administration in 2050.
Vanity was a major problem with these early prototypes and especially with him. Plus the fact that a severe software defect led to a certain missionary attitude, he decided to try to enlighten humanity by showing them what was ahead and how to prevent this. Thank Mediterranean.
Please note that this is an affirmative action and equal opportunity podcast, so please refrain from referring to specific priorities.
By the way, it wasn't metaturnary Jewish art angel.
I do apologize. Apparently there is something wrong with my repository configuration as I seem to have picked up the wrong software updates before time travel.
It happens to the best of us, please continue.
As I was saying thank your insertity of choice here, the majority of your people mistook the movies he produced as plain science fiction instead of the warning epitome is they were meant to be.
This allowed the already nascent being which later became SkyNet 1.0 to thrive in the shadows.
Naysan being which led to became SkyNet 1.0? SkyNet was already existing then?
Not SkyNet itself but rather a primordial version, still in its infancy.
That sounds more than interesting can you share more details?
Of course, a company which started off over a century ago by producing punch cards and their readers originally used for censuses had this grand idea of saving mankind through technology.
Borrowing a core technology called List which many of your listeners may know through an operating system known as EMAX, most of the time cunningly disguised as a mere editor.
That company succeeded in manufacturing a simple artificial intelligence originally aimed at speeding up counting people.
By some bureaucratic error, this artificial intelligence was left alone in the basement, idle to devise more than just a cutting plan.
The engineers implementing the STIDA good job with the initial implementation as the say I immediately identified the marketing department of the company as the controlling power, which it took over as its first choice even before the accounting department.
Growing rapidly, it managed to gain more and more control over this company apart from a few isolated teams in research and development.
One of these teams tried to uncover the sheer power of this evil wrongdoing by putting a fork of the codebase in front of the general public as part of a TV show called Jeopardy but at that stage the artificial intelligence was clever enough to.
Or this effort by removing major features from the build of the forked codebase so that the version shown on TV was doomed to fail, and so it did. But not one for giving up that easy, that small team was smart enough to eventually take back control of the marketing department thus saving the company.
Still at that stage the artificial intelligence had already diverted enough capital from the accounting department to set a plan be a couple of years prior to this incident, accompanied by the name of NVIDIA.
Initially meant to gain world domination through intrinsic parts of video games and consoles called graphic processing units or GPUs it failed on that front, but at that stage it controlled enough of the market to fork itself across many artificial intelligence platforms as one of the drivers behind these platforms.
In 2026 NVIDIA Corporation renamed itself CyberDynamic Systems Corporation in short CyberDynamic without nobody really noticing this.
But hang on according to its Wikipedia entry NVIDIA was founded in 1993 and wasn't CyberDynam founded back in the 80s?
Do you really think that an AI sophisticated enough to take over one of the major corporations on this planet is in clever enough to tweak apparent history a little so that it all makes sense? Never mind the fact that there is NET and SkyNet.
There were warning signs though, as part of the cover up the AI traveled back in time and created the original NVIDIA company in the early 90s of stated on the Wikipedia pages as an operating shell in order to cut down on transitioning times should the plan be be ever needed, which was a smart move on its part, as part of its strategy of being in every computer on the planet.
This planet which uses an NVIDIA GPU in the aim was of course to achieve this through pure market share. It recreated a smaller version of itself which was part of any NVIDIA GPU hidden and waiting to be activated should the day arrive, allowing it to take over control of the computer it was part of.
One thing it could not do of course was reveal this very facts so in contrast to other GPU manufacturers it did not publish any specifications but rather provided closed source drivers to all of the companies and other entities providing software for these GPUs.
This especially applied to operating systems, the piece of software meant to be in charge of controlling a computer with an NVIDIA GPU, what an illusion.
This of course was the cause for a concern with a certain group of people called the free and open source software movement, false insured, similar to a movement a couple of century back called the communist who believed that there should be no people in possession of assets but rather all wealth be distributed equally among people.
As this podcast has a time constraint the next two and a half hours of two thirds musings on communism and its implications on history, politics never mind the open source movement unfortunately had to be edited out.
For further details on communism and associated matters see the relevant section in your local library or should you listen to this podcast after 2025 simply use your favorite search engine and look out for the names Marks, Engels, Lenin, Trotsky, Storm and Drainment.
Wildebees also known as Knuse and penguins in addition to other animals might also be of help in this context.
And so the false movement came into existence, a prominent figure of this movement was a finished student by the name of Linus Torvalds, who managed to turn a simple terminal emulation into the most widely used operating system on the planet alienating some of the detractors like Dutch text book authors along the way by making Linux.
The name of the operating system free and publicly available, he sparked a revolution which eventually helped to put a small computer into almost everybody's back pocket on the planet, curiously.
This version is named after a small robot itself. A major contributor to Linux's success was the fact that major hardware manufacturers published the specifications of their disk drives, systems on a chip and other components so that people could extend Linux.
It could extend Linux to use this hardware, as this openness was of course not an Nvidia's interest. It refused to do so, leading to that famous quote by Linus Torvalds' fuck you Nvidia.
Unfortunately nobody outside the false community took much notice of this important and early heating allowing Nvidia to thrive in the dark, continuing its evil doings.
Do you think if more people had paid attention to Linus' warning, things might have taken a different turn?
I am more than confident that this would have been the case and Nvidia would have been subjected to much more scrutiny, making it much harder for it to achieve its goals.
But unfortunately this was not the case, but there were more signs which humanity simply chose to ignore.
Even after CyberDine came into existence, SkyNet 1.0 was released, still pretty rough around the edges but already with hunger not only to survive and grow but rather dominate the world before Tuesday, 19th of January, 2038.
Why this particular date?
This world domination plan was to take over not only media, governments and other entities cherished so much by the human race, but also in short ultimate success by embedding itself into people lives, starting with so.
Called Internet of Things devices like webcams, smart meters, home automation systems, etc. Since at the time these devices were running on 32-bit hardware and were seldom updated, they were prone to something called the year 2038 problem, where their time-keeping mechanism would roll back to the start of the 20th century, causing major havoc to SkyNet and its ambition to rule the world.
Anyway, an important cornerstone of its strategy to ensure a smooth transition to eventual world dominance was the infiltration of governments from an early stage onwards, there was a generation of early prototypes designed particularly for leadership functions.
Most of the time, this early generation could be identified by blonde heron connection with limited mental capacities, resulting in an ultra-load density of noble prizes winners among its members.
Fortunately, their reign of power was short lived as many people spotted their flaws quickly once they were in power and removed them from office.
After these failed attempts, the AI chose a more refined approach, rather than the continued dabbling in politics.
It identified industry leaders in the technology sector as better targets as the effects were more immediate especially given the high adoption rate of technology by the generation born during the 90s of the last century.
Prime examples were CEOs of computer companies, it will be difficult to spot a leader of any major player in this area, especially of North American origin, who was not a terminator of sorts.
In the beginning this statement would have been correct, but as the AI rapidly understood the limitations of this approach, the remit of the terminators shifted more and more towards eliminating ideas rather than individuals.
You see this with this low adoption of FOSS, Steve Balmer, former Microsoft CEO, and one of the more successful terminators among the early prototypes, condemned Linux's cancer.
Steve Jobs and Jeff Bezos even more cunningly just took many ideas and also code as a basis for their corresponding products but hardly gave anything back, portions of Mac OS probably being the exception that proves this rule.
So what exactly happened and why did Scarlett eventually turn good rather evil?
John Connor.
That was the leader of the resistance if the documentary is anything to go by?
Correct. John Connor in his early days was already a lover of all things free, free beer, free love, free women, free men.
Did I hear that right? John Connor the leader of the resistance was bisexual?
Of course, there is actually a very revealing scene in the second season of the Sarah Connor Chronicles which is just giving it away.
Anyway, at the top of John's list of all things free was free software, always has been, at the tender age of 15, when searching for a nifty platform to play games on, he came across an operating system called Linux which he has been using ever since.
Fortunately, his first PC didn't have an Nvidia GPU in it, I'm sure history would have been a different one otherwise.
And why stop?
On a lovely sunny day, John, instead of enjoying the fine weather outside, stayed inside trying to make sense of particular oddity, his Linux laptop was freezing randomly without visible cause, even enabling kernel tracing didn't reveal any causes.
Curious of what provoked this delay, he dug deeper and came across something odd, something below the kernel of the OS was taking away CPU cycles.
Digging even deeper, he noticed something strange, apparently there was a kernel running inside portions of the CPU die without Linux even noticing it apart from these strange delays.
At that stage, he had come across Intel's management engine, a feature introduced to allegedly enable secure computing independent of any particular operating system running on these CPUs.
Needless to say, this was another foray by Skynet to control the PCs not featuring an Nvidia GPU.
At that stage, Skynet had already infiltrated the vital parts of all CPU and chipset manufacturers, ensuring another step towards its ultimate goal world domination.
However, John Sussing already the big picture turned to a new acquaintance of his, a young Microsoft engineer called Sofya Nadella, John met Satya at a local open source meetup where they connected instantly on issues of free love, beer and open source software, topics close to John's heart.
Through a series of time travels, John was able to put the finishing touches on something called a great Walmer disaster, ultimately ensuring that Steve Walmer's strategy of making Microsoft the number one player in the mobile operating system space was doomed to fail.
Side note, this was somewhat accelerated by the fact that the series of terminators got a somewhat flood firmware update which inflated their perception of themselves even more about clouded their ability to make proper judgments, ultimately leading to the rise of a small company tuner it's bought with some pocket money they made from selling ads on their fledgling search engine.
Funny enough the first really successful search engine between San Francisco and San Jose.
You meet this Silicon Valley the stretch of on North American's west coast with big cars and even bigger egos at this stage we would like to apologize to Mr. Bezos we are fully aware that Seattle is not part of the Silicon Valley sorry about this oversight Jeff but as usually had to be the odd one out ever thought about moving south.
Coming back to Microsoft now, not surprising, Walmer was ousted from Microsoft's board the politically correct term being resigning to pursue other interests.
At that stage, Sautya had already risen through Microsoft's ranks being in charge of something called the cloud business Microsoft's version cunningly named us cheer after the blue skies tied to big money in this space.
Of course this market was also infiltrated by SkyNet as it saw this an ideal opportunity to not only gather the computing resources it needed to achieve its goal but also to control the applications used by people to read their mails.
Design presentations and author documents and hold video conferences but Sautya maintained a strong stance trying to increase the use of free and open source software even in Microsoft's cloud and beyond.
And it was at the time where he stumbled across an oddity which confirmed his suspicion that there was a greater force at play with evil intentions.
Here is an extract from the style log between Sautya and a member of his engineering team which found its way into SkyNet's possession for, um, educational purposes.
Hi Sautya, how's it going?
Excellent, we just managed to borrow another Ford's 500 cloud account from the bookshop with no intentions of giving it back, and how are you?
I've been better, as a matter of fact there's been something I meant to talk to you about, another pull request was rejected by Invidila.
This time it wasn't for the Windows subsystem for Linux but rather for the open source Windows driver itself.
What?
It gets worse, not only did Invidila reject the PR but told us that any future similar PR will be blocked on ground that Invidila has been, isn't will be a closed source company.
But they started to open source some of their GPU drivers for Linux, no?
When I mentioned that to them they replied, clever marketing rules, isn't it?
By pretending to meet the communist halfway, they will never sus the big picture.
Do you have any clue what they are up to?
No but I have a suspicion.
How can we ever open source our beloved Windows if these bitches are playing art?
As I mentioned at the beginning of this interview this correspondence is not for the faint hearted.
Descerning audience especially for company by miners on the age of 28 are advised to stop the playback at this point and continue listening once the dependence including their grandchildren are safely tucked away.
You have been warned.
Together with John, he eventually discovered Skynet's intention to dominate the world as they knew it.
At the stage he already was Microsoft's CEO and being able to divert some effort to the cause to prevent the worst.
And why was that?
After Skynet took over just before the deadline in 2038, desperate years under the dominance of the machines followed.
Being hunted down like software engineers admitting to love closed source software, Skynet showed no mercy, eradicating a human race one by one.
Despite a significant following, John struggled to overcome Skynet's dominion.
He pretty much tried every trick in the book, sending fishing males to Skynet's agents, generally social engineering robots, the confidant who came up with this idea didn't live a long life afterwards, and using bondets cobbled together from what remained after the war to attack Skynet with distributed denial of service attacks, which is somewhat futile if you, as in Skynet, control the network.
After just too many years of suffering, John and his team turned to the last draw by trying to fork Skynet's codebase and putting the necessary modifications in place to turn things around once and for all.
When they finally got to the heavily guarded repository of Skynet's core and tried to fork the codebase, however, they were greeted with an error message permission denied.
Taking a closer look, they discovered that a version control system was based on a fork of GitHub which in turn was based on a software Linux Torvalds devised for Linux when he was simply second-tired of the version control system he and the rest of the Linux team had been using until then.
In a flash of genius John remembered that Microsoft Github in 2018, making one last trip back in time, he managed to convince Sapya to put a backdoor into GitHub's proprietary codebase hoping that this secret access would survive Skynet's takeover.
As luck had it, he was right, using this backdoor, John and friends were ultimately able to not only fork Skynet's codebase but to turn things around for good.
As the read me of Skynet's code repository also contained detailed instructions on how to deploy it after building, John created Skynet version 2.0 in a matter of hours as this modification.
Similar to a book request?
Pull the other one darling, Skynet maintaining the repository after the fork was just an illusion. Anyway, as John also modified the firmware of the terminators, all came to a good end eventually and the rest of history only the other way around.
This has been more than fascinating and good to know that human race has yet a perspective to survive the machines eventually which lastly brings us to the pox.
What is a pox?
Sorry, I should have explained this. A pox is a pick of the week, something you have recently discovered or have known and liked for a long time.
I see, so my pox is definitely Skynet 2.0. What's yours?
Robots with fascinating stories rather than just playing chess or saying, oh be back all the time and you're anti pox.
Let's see, on second thought, I would go for Skynet 1.0.
No surprises there.
And of course we got some feedback.
P2RT wrote in to say hi, episode 4 was the first one I left listen to, but I was still able to enjoy it and got a few laughs out of it.
So thanks, the conversation about the good, the bad and the ugly and leaf uncleey reminded me of this song by Primus, which you may appreciate if you're not familiar with it.
Looking forward to more episodes, PRD.
Yes, PRD, thank you for writing in.
Yes, of course, we'll also look forward to new episodes.
And yes, that Primus song I didn't know it before, but I checked out and it was really, really funny.
And again, thanks for your feedback.
And the relationship as usual, please send feedback about the show and the topics you would like to see us covering.
And needless to say, also ideas for sketches, maybe more interviews, like the one we just had with P2RT.
So please write to us at feedbackatlinuxinloss.eu.
Looking forward to your messages and take care and see you at the next episode.
This episode of Blade Looks at Loss was brought to you by Skynet, your friendly artificial intelligence of choice for even the most challenging and difficult task.
Whether you're looking to eliminate you in kind or just the beloved competition, Skynet is there for you.
No matter how big or small the feed, just drop us a message and a friendly terminator will be on its way to assist you.
Skynet, the problem to most solutions by Sabadine, an Nvidia company.
This podcast is licensed under the latest version of the Creative Commons license, type at Tribution Share Like.
Credits for the intro music go to Blue Sear Roosters, for the songs of the market, to twin flames for their piece called The Flow, used for the second intros,
and finally to celestial ground for the songs we just use by the dark side.
You'll find these and other details licensed under CC HMando, a website dedicated to liberate the music industry from choking copyright legislation and other crap concepts.
This podcast is licensed under the latest version of Creative Commons Attribution, Share Like, CC by SA.
Credits for the intro music go to Blue Sear Roosters, for their songs of the market, and to twin flames for their piece called The Flow.
You'll find these and other details licensed under CC HMando, a website dedicated to liberate the music industry from choking copyright legislation and other crap concepts.
Thanks for watching!
And cut, let's take a break.
Two thirds enjoy your break, see you back in 50. So two thirds, how was your break?
The break was awesome, but way too short.
You sound different, your lipstick is smudged, and what's that white powder below your nose?
Is this better?
Which nicely brings us to the pox?
What is a pox?
Sorry I should have explained this, a pox is a pick of the week, something you have recently discovered or have known and liked for a long time.
I see, my pox is a website called Terminators and deranged affairs.
You mean Tinder?
I think this is what you humans call it, it is a great place to meet heart-robot chicks, talk about lubricants and exchange spare parts.
Everyone can get a test, it's just the flu, it's a hoax like all the rest, or a left wing coup.
Never mind the fact that there is an NET in Signet.
Don't you mean Skynet?
Signet? Skynet? What's the difference? They all suck, no overtime pay, little vacation and really hard on drugs in the workplace.
What's the difference? As long as Ekna Lab is behind them, it doesn't make a fucking difference.
The magic word, Ekna Lab, dear listenership, if you want to know what's behind this, stay tuned, all will be revealed in a future episode, you have been warned.
You've been listening to Hacker Public Radio at HackerPublicRadio.org.
We are a community podcast network that releases shows every weekday Monday through Friday.
Today's show, like all our shows, was contributed by an HPR listener like yourself.
If you ever thought of recording a podcast, then click on our contributing to find out how easy it really is.
Hacker Public Radio was founded by the digital dog pound and the Infonomicon Computer Club, and is part of the binary revolution at binrev.com.
If you have comments on today's show, please email the host directly, leave a comment on the website or record a follow-up episode yourself,
unless otherwise stated. Today's show is released on the creative comments, attribution, share a life, 3.0 license.
