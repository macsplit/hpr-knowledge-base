Episode: 4009
Title: HPR4009: Reolink CCTV Cams
Source: https://hub.hackerpublicradio.org/ccdn.php?filename=/eps/hpr4009/hpr4009.mp3
Transcribed: 2025-10-25 18:34:49

---

This is Hacker Public Radio, episode 4,09 for Thursday the 14th of December 2023.
Today's show is entitled RealLink CCTV Camps.
It is hosted by Operator and is about 33 minutes long.
It carries an explicit flag.
The summary is Operator Talks about RealLink CCTV Camps.
You are listening to a show from the Reserve Q.
We are airing it now because we had free slots that were not filled.
This is a community project that needs listeners to contribute shows in order to survive.
Please consider recording a show for Hacker Public Radio.
Hello, another episode of Hacker Public Radio with your host Operator.
Apologies, probably one last time for the audio in the car.
Hopefully I can tune some of that out.
This one's going to be about security cameras, realLink, and kind of revisiting that since
my hardware changes here recently.
This all started way back when object recognition started to be a thing with yellow and dark
net.
This was before you had rain, before you had any kind of services like that.
Now it's all cloud-based or whatever.
So I had Python scripts, Python 2 scripts, leave it or not, to process images captured
using Zollminder, which is a very wonderfully designed security camera software.
If you maybe had hundreds upon thousands of camera feeds and you wanted to really just
hardcore get to the nuts and bolts of security cameras and motion capture and all that stuff,
you would probably start with Zollminder.
I want to say they're working towards a more user-friendly thing where somebody else has
taken their stuff to the next level or one of the coders is trying to do his own thing
because it's a little barrier to entry type of thing.
Anyway, Zollminder is for security cameras.
You can set it up not too long and I had it, I had a shell script that would pull images
down and look for a motion, I think.
It would look for motion events in Zollminder, pull a random sample of the motion, so there's
24 frames.
These are impacts.
These are images.
Full, let's see, it's a 1080p images.
Anyways, I'm going too far into it.
I'm going to go kind of quicker, less rambling in the car here.
It would pull down images and at first it was processing every single image and usually
with image captures or motion capture, they capture a pre-capture, so it sees the motion
cool.
I'm going to give you five frames or five seconds before the motion, so you can see the
motion leading up to the motion if that makes sense.
If it doesn't detect the person up until half a second or two seconds into the frame,
then you're going to just see them at the top of their head or something.
A lot of them built in have a frame buffer, so they'll once they detect a motion or they
detect a person, then they'll back up however many seconds or however many frames and
give you the option and dump that out.
First I had it processing every single frame and I said, this is week and slow, it takes
way too long to get an actual disposition without using GPU.
This is before local GPU processing was very easy, you had tens or flow and very smart
people doing that stuff and I was not smart enough to train my own models or anything.
So I got it down to about, I want to say 10 seconds per image using a very large model.
So the backup, real quickly, sorry, yellow, darknet, you would feed it in a mish and then
you would mash it up against a model, train for whatever.
The model I had was a pretty big model because this tiny model was slightly not as good
and sometimes it wouldn't pick up pets or people and it was also really not that much faster
and it was like the difference between 10 seconds and like nine seconds or something.
So I said mine as well, you know, eat the extra second and use the full model but it ended
up taking 10 seconds on a regular CPU.
I didn't have a dedicated GPU to use like yellow with the GPU built in, Nvidia has a, oh
what's Nvidia's thing, help me out now, Nvidia has a framework for working with GPU processing
anyways, CUDA, Jesus, CUDA supports with darknet, yellow, so you can feed it to a chipset
that the video just said the supports CUDA and you can significantly reduce the time it
takes to, you can load up the model and my onboard GPU wasn't fast enough to load up even
the tiny model into its RAM, you need it like eight gigs or like, you need like 10 gigs
or something, the video RAM to load up the model into your GPU.
So I was using CPU to take in 10 seconds, so I picked random frames and then I had like
all these straddles built in because like bugs and spider webs would get on the thing
and then we're all in like a rain and then like when it would rain, sometimes it would
back up the motion for like hours, so I'd wake up and then like you know like two hours
later I get in a bit that like you know a bug goes on the screen or something and it
would be from like three in the morning, so I knew that if it rained, you know, I knew
that if I got a late, very late notification, it was because it was raining or there's
a spider web, so it was kind of a mess managing all of that manually, it was just scripting.
I had thresholds built up, sometimes the feed would get all scrolling so I'd have to like
actually hard reset the, not hard reset but power cycle of cameras for it to fix itself.
I don't know if it was this old mind or feed thing or what, I never figured it out.
So there's all this weird crazy shit in my old mind or scripts.
Nowadays, fast forward, 95 years later, maybe 10 years later, not even that, now real
link entry level old cameras have built-in object recognition and it is pretty good.
The cameras I first purchased were, I think they were HD, I don't know what HD is, it's
not 4K but it's against up to 420, they're 1080, I want to say, the lighting on it wasn't
great but, you know, it worked for dark night and yellow and I used it and then when I went
to go updated for Python 3, I realized, you know what, this is long in the tooth, this
takes too long and it can screw it up when it rings and there's spider webs, it gets all
confused.
I just need to simplify this and get rid of this whole thing because I'm not going to
even try to update this code.
Looked around, realized it, you know, we had come a long way and we got your rings and
your Googles and your object recognition built into the cameras themselves, they're
trained on a very small data set, my problem is mine was trained on everything so like
the first time I got it, it was like, 38% vote, like great, it works but, you know, there's
not a vote in my driveway so it's, you know, I got a elephant, you know, they call my wife
a pet or dog or something, so like, I had to like filter out anything that wasn't person
vehicle.
Truck, cat, dog, pet, whatever, so that's one reason why it took so long because I didn't
have a fine tune model or even train, if I knew how to train models, I got to train it
to my actual images of people and pets and stuff but as I was supposed to do it, but
they took all the work out for you and I don't even know if they all use the same model
based on the different field of view.
So field of view is another thing, I screwed up on, so the second set of, you know, first
one is real links, darknet, yellow, all home grew, home grew, like did it all myself using
massive models on the CPU based architecture.
There's much faster ways to do it now, you can spend like 60 bucks and get like an offload
like film to sell or film card or something for Google that'll do like GPU sort of stuff
like lightning feeds, I think they're like, you know, anywhere from like 80 to like 100
something bucks and you can send it to that.
It's essentially like what they were at the kids were using for cracking cryptocurrency
stuff using like a specific, it wasn't CUDA, it wasn't GPU, it was called whatever, you
know, if you're going to like, you know, these are like R and like do a bunch of number
crunching or something, you would use these little special devices that were like made
for crunching numbers that were CPUs, they were like, for how are they called, F cells
or F somethings, but you got like daisy chain together, you know, crack, I don't know
where, whatever happened with that, why they're still doing GPUs.
But anyways, I started there, the second set of cameras I got were on sale of course,
but they were, the field of view was zoomed in quite a bit and my wife had mentioned
something about it and I got them and I was like, man, you know, I just, I set them
up.
I got them drilled in the house and, you know, it actually is kind of nice because it
like it'll zoom in and it'll like, you know, if it's zoomed in a little bit, you'll,
you can see better farther away, but you kind of need that fish eye aspect to get more
field of view because, you know, our front door is a front door and it's not a one-way
system, so you can come in at 180 degrees, right?
So like people could essentially, like, just walk straight to the front door and not walk
in the driveway and never be on camera.
So, you know, that sat for probably five years, we got our deck, or not deck, our sighting
done and of course I put the cameras right back up and then I also know what, you know,
these cameras are the right field of view or two zoomed in and then I realized I purchased
the wrong ones or the incorrect ones.
So they have like, real link has the same, you can't even buy them on real link sight.
They only sell the four, what are four millimeter ones.
The ones I bought were on sale and they were six millimeter on their website and then
they have a 2.8 millimeter which is basically a fish eye lens.
I'm not 100% sure, but I think going back to my original training thought is I'm thinking
maybe the models are different.
So there's two problems with what the set of I have now, particularly in the backyard
of the East.
The images is sorted, right?
So if the images is a story, do they train the images on, do they retrain it and fine-tune
it for the image, distorted images or is it all the same model, do they put a different
model on it?
I'm assuming since the firmware is the same, the models are the same and maybe it's
trained on a very specific small set.
I don't know what magic it does, but I have seen, you know, real-time stuff.
This was five years ago, you know, I was watching real-time people do real-time on like
a foam of people and face detection stuff.
So I think it was a small enough model in the right kind of no-health and not a giant
massive, like one gig model or whatever I had.
So I don't know how to do it, but one thing is a distorted image, right?
If it's fish eyeed, you're not going to be able to detect people as easily without
a specially trained model, or maybe I'm crazy and stupid, maybe the model works and
knows that it's, you know, a distorted image, I don't see how that's possible, but maybe
models don't, yeah, image models don't care.
So you got a distorted image, not on top of that.
The backyard is a different case because I don't necessarily care about the backyard.
It's on the corner of the house, again, at the door.
So I guess in hindsight, what I should have is I should have it aimed from the door
to the, from the door to the outside world, I'm thinking that the security camera should
be at the door and then facing away from the door, so you can capture, you know, you
do a fish island and you can capture it.
The problem with that is it's not very convenient for me to run a cable with a mic all the
way to the kitchen.
I don't even know how to do that, honestly, I have to run it to the attic and then back
down through the kitchen, going through like 500 feet of cable, so that's probably why
I didn't do that.
So I have it on the corner, just poking a hole through the wall, I have it on the corner
aimed at the door.
The problem with that is that these are white kind of white screen images, they're not square
there, you know, traditional, whatever, that's what I appreciate it is.
And I got a lot of white, I got a lot of the backyard, which I don't care to have.
I would rather have the camera aimed down more at the ground and capture more below me.
It actually almost catches like a right below me and then something, so I actually have
to kind of be behind the camera in someone to get out of it's view, completely.
So not only is it nighttime, and it looks like two, and the frame rate is not great, it's
a lot better now with this camera on the same price, relatively, they're like, I mean,
I can see up to like 80 bucks.
The image is, you know, not only is it distorted from four millimeters, so all it's from six,
I went from four millimeters, then I went to six, which is zoomed in more.
And then I went to 2.8, which is kind of a fisheye.
So not only is it kind of a fisheye lens, I have it cropped up sideways.
So my question is, is when it's doing object recognition, and I haven't tested it,
comparatively, what this guy is doing, on the side of the road, you said, you know, I
said of being distorted, the image is also sideways.
So how does your model, when it's scanning, how does it know that the human is like walking
sideways, traditionally, maybe I'm wrong again, it doesn't care about the distortion,
and it doesn't care about the orientation of the human or the pet or the vehicle.
That's kind of requires some testing, but I want to at least, there's not going to be
a vehicle in the backyard, hopefully.
So I actually turn that off, or I turn this into the very high, or whatever it is to say,
look, look, there's never going to be a car in the backyard, but go ahead and give me,
log when you think there's a cat, but don't alert me and send my wife an email in the morning
because the cat's been in his room, the neighbor's cat's making a round.
So there was initially some noise around the heads, but I do wonder, I need to test the backyard
for humans at least. So there's some issues there, as far as setup, man, going from the
real link stuff to the new real link stuff, it was stupid, easy. I had one live, I kept one live,
and I just kind of looked at it, and it was like, it's kind of how I had this one setup,
and I kept everything default where I could, but yeah, setting them up was stupid, easy.
I hear that they're all kind of the same chipsets nowadays, it's just different UI,
probably like two or three different chipsets for users, level entry stuff, but yeah,
as far as setup goes, yeah, really think about it. I did some more tuning in the front yard,
because I think I got a lot of motion events because of like spider webs and stuff.
So I don't think that's going to help with motion, what you can do with spider webs,
which I probably talked about before, you can get mothballs and drill them, drill like a little
container in the side of your house, right under the camera, or at least as close as you can,
you want to have that around the camera, don't touch that stuff, it's nasty, mothballs are gross,
like really bad for you, like wash your hands, all at some,
so you check on them every once in a while because they do tend to evaporate,
and if they're, if they're waste, they're waste, they will, and should kind of evaporate.
That will help with the, with spiders, and there's also some tips around placing them.
I think you want to place them under something, you'll get less critters flying around,
I feel like if you place them under something, you also won't get the ran effect,
because the rain won't be close enough, so it's objects that are very close, like,
I don't think there's any way to get around the whole like,
insects flying in front of your camera, the only way to do that is to put like a piece of lexan,
or even glass between the camera and your object, but you want to put a far enough way,
is you want to put a far enough way to where it's not going to get gross and screw up the picture,
but you're going to have to clean that lens, you know, the big plate of glass you've
put in front of the camera, if you do that, so I don't really know how to get around the bugs flying
in front of your camera thing, but the mock balls do make a difference. I used that to go out,
you know, scrub the lens, scrub the lens, and generally speaking, I had to do that all that much,
even in the past couple of years, but every little bit helps, but I'm pretty sure it's the mock ball stuff,
it helps. There's also like a trigger, so you can have a trigger on motion,
and then if it detects a person, it will turn on the like the floodlights.
There's a delay there, so you know, it's 90 visions on, it detects motion, once it detects motion,
object recognition kicks in, object recognition picks up the thing you want to say is an object,
then it turns on the floodlights. Within that transition, it has to, I think it instantly
adjusts to a nighttime floodlight profile, essentially, so it just saves, hey, this is,
you know, once you turn on the lights, set to this exposure, set to this frame rate, or whatever,
to get around, having to wait for it to like figure out what, you know, what the optimal settings
for the floodlight is, it's gonna know, it's nighttime, you can't see anything, you turn on the light,
and it's moving on, but there's like a, I want to say it's a good 500 milliseconds, half a second
of transition time, so if your target is not in frame for very long, and it takes, you know,
a second, half a second to detect a, to detect a human, and then another half a second to turn
on the floodlight and see the human, you might lose your objects within that time, but
the nighttime stuff is still not great, if you don't want to be caught, as a robber at night,
just move quickly, and you're like, well, that's like around circles, where you could wear a
mask or whatever, but it's the holding still is what's gonna give you the better picture,
but I will say, once the floodlights turn on, you get a good shot, you get, you know, close to
high quality, it's not 4K, like it's actually supposedly allegedly capturing, but it's good,
it's a good clean image, the problem with at least real link, I have a script on my GitHub
that will pull down the full size images, there's no way outside of the real link app,
even if you fire up burpsweet, it's like encodes, it's like this, there's like one post on the
internet about some guy decoding, the encoder for the, for the downloading of the, of the images
or whatever, they say it's to protect it, because like there's, it's not on, SSL is not on by the
phone, so they're like, oh, well, you know, we're doing it for security, so that like, you know,
somebody's trying to sniff the wire, they're not going to be able to like sniff your streaming packets
of, you know, your videos, so that's why we have it all like encoded, it crypted quote unquote,
it's not, it's just so that you can't easily pull the data off, using like burpsweet or something,
you have to use this like, you know, GitHub project to like reverse engineer the protocol,
binary protocol thing that they use, so it's, it was frustrating, they do fortunately,
real link has an API, it's horribly documented on purpose, to pull down the full size images,
so you have to like, send a very, very specific formatted, like you can't even like put an extra
carriage return in there, or throw up, and that's how all these like IoT devices are, it'll,
it'll throw up an image, throw up an error, the way I ended up getting the API to work
was just by like sending a tiny, tiny, tiny example request, finally went through,
because you can't, since you can't record, right, because it's all encrypted or whatever,
you can't record an example session, but luckily, somewhere had, somebody had a GitHub project,
it was like, some kind of tool to download something or check the status of something,
and I managed to finally somehow find one example post request to the, to the actual camera itself
to request a video download, and all I had to do was kind of reverse engineer that to give me like,
the right size image, so that's how my GitHub, you can give it the, well, IP,
username, password, and it will give you the link to the full size image, or the full size,
the full size video, and then it will use like, W you get to pull it down, it's not multi-threaded
and it's gross, but it works, I mean, it doesn't require this stupid, basically, adware that is
the real link app, and even with the real link, it does the same thing, it's encrypted, encoded,
this stupid binary protocol crap that they claim it's from security, but it's because they don't
want you to use another party thing, and they want you to buy their DVR and all that crap,
so with my script, and I like $89, I would probably stick with the four millimeter ones,
the fisheye is a bit much, unless you have a very small closed quarters area like a office,
my ears are popping cold, and I'm going down or up, I don't even know,
there is a, there's a wanky road, so I don't want to hit any deer or anything,
um, between the GitHub project and a, I got indoors, I would use the 2.8, I do like it for
the backyard, the 2.8 for the backyard, but it's, it's gotta be hit or miss if it
is going to capture everything, it's supposed to capture, but anyways, so with the real link
four millimeter, and my script, you pretty much have free rain, so you can put in a email address,
and it will give you the low quality video version of the video and ship it to an email,
ship it from email, now Gmail has default, by default, they don't allow anybody to log into
anything, like this guy passed me, because he's an Erie, I do not want to ride that fast,
I don't push the 25 mile an hour road, so like, even with like the low quality, it supplies,
satisfies my wife's requirements, and then I'll tell her, like if we see something really
healing, it's like one time I cut a tree down, the cat was on, I was outside, like on a leash,
and I don't ask questions, but it's freaking hilarious, and she wanted the full size video,
so I had to like download all the videos for the entire day, because my script only does it by day,
so she pulled that down, I used it for like Halloween to try to find a particular
kid's costume, and then I realized it was kind of creepy, that was like, you know, like recording all
the children in the neighborhood, but you know, we're in this surveillance state now, so, excuse me,
all bets are off, so between the my script and the embedded support of these real links,
you don't have to pay for any kind of, you know, cloud-based whatever, now if you want an idiot proof,
yeah, buy your ring, buy your ring and pay the $5, $3 a month or whatever they want,
for all that, what I would like to see and have is a plate reader, which is fancy optics with
fancy night vision cameras that capture high-speed targets that are especially set up for high-speed
targets, like plates, the problem with capturing, going in some antics here, my idea was to put a
a license plate reader at the bottom of our driveway and face it one direction, and then put a
mirror on the other side, and the mirror would project the other direction, and then I could capture
two different directions down the street with a single image, a single camera, I wouldn't have to
face a camera in each direction, that was my idea, it has a creepyness factor to it, it also
doesn't really catch up much because our house is off the beaten path, so we have like three
entrances to our neighborhood, so if I really wanted to do it, I would get a steel, some power from
from somewhere in the front of the neighborhood, mainly the lights, then I would run that to a
little crouching tiger-hidden drop, and power an air card, so it would be like, you know, $20
a month for each site, so that would be however much, 60 bucks a month probably, I don't even know
what the cheapest air card you can get nowadays, probably can't even get them for a list of 60 bucks each
now, so that's what I ultimately wanted was to be able to track plates and cars, and even
people in the cars, so I could say like, you know, oh well that's all there, somebody goes on
next door, the neighborhood forms, and it's like, I saw a red truck, and you know, the guy was drunk,
oh yeah, that truck has gone here and there within this many times, you know, based on whatever
they live on this side of the street, and then you come and go, so they're actually living here,
or telling me, hey, here's the plate number, for like 20 bucks a year, you can look up plate numbers
or whatever, like for these data brokers you can buy, that type of stuff, but I could have it
synced in to where, yeah, it would be fairly easy for me to just like search a license plate,
and get back information on, you know, the sketchiness of somebody based on their plate, but
that was the other idea I had, which never really came to fruition, because not a lot of stuff
happens in our neighborhood, but, you know, we got all these about that, I don't know, but that's
pretty much it on the camera stuff, don't pay monthly fee, just do real links and be done with it.
Anyways, I appreciate, oh, 128 card is far too much, I think I'm holding like
three months, four months worth of data, you don't need 128, you might need 64, even 4K,
pedestal really depends on what you're setting for the default settings,
it is no way, I was going way back, I was going like four months back, I don't know what it is now,
but yeah, 128 is way too much, but they're cheap, they're relatively cheap, and these
along as they're, you know, protected and you're not in a very harsh environment, I'm in Georgia,
where it's pretty, pretty, the weather is fair, just gets warm, we don't get cold, the cold is
apparently what the kills are, so, you know, if you're in a cold area, you probably want to get a nice
micro SD card, instead of whatever the cheapest one is, I got like a, you know, your stainless,
whatever you're decent, now you can get like aftermarket sand discs that are like
Chinese-yum from like wish.com or polybobler, all the express, but yet, you know, I just don't,
it is not worth it, like, especially if it's something you care about, like security or your
Nintendo switch, and you put some back-to-do SD card in there, and it like throws up and
starts breaking you, and then you're spending like half a day trying to figure out why it's not
working, and you realize, oh, the SD card is like flaking, and it gets all high, and you start
using it, so, you know, it's worth the extra, you know, $20 you're going to pay for a real SD card
that's like from a decent manufacturer, so that's the only warning sign I'll give you there,
um, you know, use a, I use a wire brush, but uh, use a co-hanger with, uh, what do they call that?
It's stuff that gets played with, it's a little fuzzy wire, fuzzy wire, I don't know, it's
usually green, whatever that wire is, and I'll wrap it around there, and it's like a light little
fuzz, fuzz thing, um, the dust to clean off the cameras, if the spider webs get in there, so, um,
but they weren't really, they're not really that bad once you get the
beyond mothballs form, um, that's pretty much it, uh, you're still with me, I'm still up, and hopefully
I'll be able to make it home, ladies and gentlemen, you have been listening to Hacker Public Radio
at HackerPublicRadio.org, today's show was contributed by a HBR listener like yourself,
if you ever thought of recording broadcast, and click on our contribute link to find out how easy it
leads, hosting for HBR has been kindly provided by an onsthost.com, the internet archive and
rsings.net. On this otherwise status, today's show is released under Creative Commons,
Attribution 4.0 International License.
