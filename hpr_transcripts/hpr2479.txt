Episode: 2479
Title: HPR2479: Intergraph workstation
Source: https://hub.hackerpublicradio.org/ccdn.php?filename=/eps/hpr2479/hpr2479.mp3
Transcribed: 2025-10-19 03:52:01

---

This is HPR episode 2479 entitled InterDraft Workstation.
It is hosted by KWP and is about 29 minutes long and carries a clean flag.
The summary is my reveal of my InterDraft Workstation.
This episode of HPR is brought to you by archive.org.
Support universal access to all knowledge by heading over to archive.org forward slash donate.
Support universal access to all knowledge.org.
Support universal access to all knowledge.org.
Support universal access to all knowledge.
Support universal access to all knowledge.
Good day and welcome to Hacker Public Radio.
My name is JWP and I want to talk about one of my projects that I did at work.
I've been going through my old work servers and they typically run until I can't update them anymore.
And then they sit until I have a bit of free time.
And so I have an old intergraph box and I put a new premium for board in it about eight years ago.
And I may have even bought that board used off of eBay or something.
I can't remember.
But I had the recept tape to the box and I had done an expense statement from work when I had updated the thing.
And I had been running a Cento 6.0 on it and I think I had gotten it up to 64.
And then somehow with the repos they weren't working.
I noticed that while I was playing with it that the box had all the symptoms of a week PSU or power supply unit.
And so I had to take out the DVD drive which was the IED and the graphics card so that it would have a little more power to work.
Or there was a loose connection on the ground or something in there wasn't working.
And I wasn't and I found that in the past that with the easiest way that if you not getting enough power
is to start disconnecting things until you get down to what you had.
So now there's just the motherboard with the memory and two hard disk in the box.
So what about in a graph?
So in a graph corporation is an American software development and services company.
It wasn't always so.
It currently provides enterprise engineering and geospaths geospaths specifically powered software business.
So it does something with geospaths businesses and governments and organizations around the world.
In a graph operates through three divisions hexagon ppm hexagon safety and infrastructure and hexagon geospactyl.
Its headquarters is in Huntsville, Alabama and 2008.
Integral was one of the hundred largest software companies in the world and 2010.
Integral was acquired by hexagon AB.
Integral was founded in 1969 as MS Consulting Inc. by former IBM engineers who had been working with NASA and the US Army in developing systems that would apply digital computing to
real-time missile guidance.
And the company was later renamed Integral in 1980.
Thus that's why it's in Huntsville, Alabama is because the US Army missile thing is in Huntsville, Alabama.
Okay, so that's just a little bit of that.
In 2000, Integral exited the hardware business and became purely a software company on July 21st, 2000.
It sold its intense 3D graphics seller divisions to 3D labs and its workstation and server division to Silicon Graphics.
And Silicon Graphics of course last year was purchased by HP.
So the company's incorporated Smart Sketch, which is a drawing program previously used for pinpoint OS and EO tablet computer.
When pin computing didn't take off, Smart Stretch was ported to Windows and Macintosh platforms.
I have a TD 300 or TD 400, it's just a shell now.
So all of the guts and that thing have been replaced over the time.
But the TD 300 and TD 400 personal workstations at the time from Integral, this is back in 95, 96 where one of the first to offer 3D capabilities equal to or below prices of PCs configured as 3D workstations.
The company said the TD 300 and TD 400 were available for starting at $54,495 back in 1997.
So I have the case, the old TD case, because I think it's still pretty interesting.
And as I said before, at some point in time I put a P4 dual-core in it.
I date this probably between 2004 and 2007 from Wikipedia on pinium 4.
And so I had to think a little bit about what I was going to do with it.
So I have the Ubuntu 32-bit box at work that is on that small form factor thing.
And I'm pretty happy with it.
And so I said, what do I need?
And I said, well, my Susa Enterprise 12, my Susa tumbleweed and my Susa leaper all on my Hyper-V box.
And so I said, what am I missing?
And I just came back from a Red Hat training where if I could have VPNed into a rail box, physical rail box, and played around, it would have been really, really great.
And so I was really leaning toward SintOS, even though I know from the past that they have real upgrade trouble.
So if you let that box sit and try to come back and update it, the repositories all change, and it's a pain in the butt to get all that to work.
Now, when I went to the SintOS website, it seemed that they're moving more and more and more people are using the cloud with that stuff.
It's a cloud server, and I'll talk about that later.
And so I picked SintOS, so I didn't have to worry about trying to manage the internal subscriptions at work and everything.
And so what is SintOS? And SintOS is a community enterprise operating system.
It's a Linux distribution that attempts to provide free enterprise class community supported computing platform functionality compatible with its upstream source code from Red Hat Enterprise Linux.
In January 2014, SintOS announced the official joining of Red Hat while staying independent from rail under the new SintOS governing board.
In July 2010, SintOS overtook Davian to become the most popular Linux distribution for web servers with almost 30% of all web servers using it.
And Davian retook the lead in January 2012.
So I know in my SAP world that a lot of people are using SintOS for their tests and development and sandbox systems rather than paying the subscription fee to Red Hat.
And so if it's happening in the SAP world with proprietary software running it on that, it's almost identical.
Again, in 2014, Red Hat announced that it was sponsored the SintOS project helping to establish a platform well suited to the needs of open source developers that integrate technologies in and around the operating system.
As a result of these changes, the ownership of SintOS was transferred.
The trademarks were transferred to Red Hat and which now employs most of the SintOS head developers.
However, they work as part of Red Hat's open source standards team which operates separately from the Red Hat Enterprise team.
A new SintOS governing board was also established as I said earlier.
In SintOS, developers used Red Hat source code to create a final product which is very similar to rail.
Red Hat's branding and load codes are changed because Red Hat does not allow them to be redistributed.
SintOS is available free of charge and technical support is primarily provided by the community via mailing lists, web forms and chat rooms.
SintOS version numbers and releases older than 7.0 have two parts, a major version and a minor version which correspond to the major version and update set of rail Linux.
Rail used to build a particular SintOS, literally for example, SintOS 6.5 was built from the source packages for rail 6, update 5, known as rail version 6.5 which so-called point release of rail 6.0.
Starting with rail 7, the SintOS version numbers also include a third part that indicates the month stamp of the source code the release was based on.
For example, version 7.0.1406 still maps to SintOS release to the zero update of rail 7 while the 1406 indicates the source code released on the dates of 14 June 04.
Using the month stamp allows installation images to be reissued as of July 14 oncoming container and cloud releases while maintaining the connection to the base version.
Since 2006, starting with rail 4, which was formerly known as red had enterprise Linux 4.0 update for red had adopted a naming convention identical to that used by SintOS, for example, rail 4.5 or rail 6.5.
There are alternate arch releases or released by the alternative architecture, special interest group, what they call the alt arch sig to support
and the inside of these different architectures are.
My previous experience was Sint, which was only when I had it, is that if you don't hit the updates and you don't keep your eye on it, the mirrors will change and you can't find a new mirror and it's really, I haven't ever figured out, I think maybe that'll be my next goal is to figure out how to run one of these things really long term with the mirrors constantly changing and stuff.
If you have a rail subscription, you just connect to the satellite server inside your company and then all the updates and everything happened through that satellite server from red had.
But if you're doing a community, then he has to find a mirror and sometimes the mirrors change and it's not like a boom to where you can go to the Ubuntu repository or Mint, you could get something from Mint or Davian.
I find it to be very, very extremely different with how to get the packages and once you find a good mirror, it's okay, but I had to, it took a very long time for it to try and define the fastest mirror and sometimes it would connect to something somewhere and I would know where it was going.
So the repositories, there's three main repositories and these are known as channels and they contain software packages.
So there's the base and the base packages that contain CentOS point releases and it gets updated when the actual point release is formally made in the form of files and images.
And then there's updates that contains packages that serve as security bug fix or enhancement updates issued between the regular update sets or point releases.
Bug fix and enhancements updates release this way are only those unsuitable to be released through the CentOS fast track repository described below add-ons.
Add-ons packages required for building packages to make up the main distribution but are not provided by upstream.
So this is the three things that you get, but you can enable other things. So CentOS provides several additional repositories which I have never activated that contains software packages not provided in the default update.
These repositories include the following CentOS extras contain additional functionality to CentOS without breaking its upstream compatibility.
CentOS plus contains packages that actually upgrade certain base CentOS components changing CentOS so it's not exactly like the upstream provider's content.
And then they have CentOS testing CentOS plus CentOS extras offer packages that may or may not replace core CentOS packages and they're not guaranteed to work properly.
Now most important is the fast track CentOS fast track contains bug fix and enhancement updates issued from time to time between the regular update sets for point releases.
These packages release service very close candidates for inclusion to the next point release and this repository does not provide security updates and does not contain packages unsuitable to uncertain inclusion into point releases.
And then they have a CR continuous release thing that will have packages that are always in a debug info and a contrib channel which also contains packages contributed by CentOS users that do not overlap with any core distribution packages.
And then they have this thing called software collections which provides software newer than those provided by the base distribution and I know a lot of people use these collections.
So during my setup I had to do something with LVM. The drives that I have are the two western digital 320 gigabyte disk one was very very hot to the touch and so there wasn't much room between the drives and so I moved it up and mounted it into a three and a half.
There are all three and a half inches wide but some of them are full length three and a half and there was another one that was a half height.
So I moved it up into one of the half heights and just made the screws sort of match and they cooled that drive off pretty much immediately.
And then the Linux and Linux, the logical volume manager, LVM is a device, mapper, target that provides a logical volume management for the kernel.
Most modern Linux distributions are LVMware to the point of being able to have their root file systems on a logical volume.
Okay, a guy named Heinz, Maul's Hagen wrote the original LVM code in 1998 taking its primary guidelines from HPUX volume manager, Kotly that makes my heart feel good because I sold a lot of HPUX boxes over the years and to see something from HPUX actually make it into the Linux kernel really powerful for me.
LVM is used for the polling purposes creating single logical volumes of multiple physical volumes or entire hard disk and that's what I did.
Somewhat similar to raid zero but more similar to a jbot allowing for dynamic resizing and so that's the one I did and so may managing large disforms by allowing disk to be added and replaced without downtime.
That's what most people do in the server room there on small systems like a desktop instead of having estimate having to estimate at installation time how big a partition might be LVM allows the file system to be easily resized as needed.
So you can go back into the partition manager and sent us and just make it bigger but I found that after I did everything that everything was properly set up and that it worked really really well.
And if you read if you reboot it which you part it and look at it it's got like the LVM thing and that takes up a few less than a gigabyte of space and then it's underneath our XT4 partitions underneath there and so it's so performing consistent backups by taking snapshots is also a part.
LVM can be considered as a thin layer of continuity and ease of use for managing hard drive replacement, repartitioning and backup a software layer on top of the hard disk and partitions which creates an abstraction so like I said when I used to parted there was a small partition with I guess the LVM stuff and then there were two XT4 and there was XT4 partition underneath.
And so basic functionality so you have volume groups which are called VGs and these can be resized online by adding new physical volumes or ejecting existing ones and then you have LVs or logical volumes which can be resized online by.
Con, wow that's a word. Con, cat, concatenating, concatenating, extents onto them or truncating extents of them. LVs can be moved between PVs, creation of read only snapshots of logical volumes, LVM1 or rewrite snapshots, LVM2.
LVGs can be split or merged in and sito as long as no LVs span the split. This can be useful when migrating whole LVs to a form of offline storage so take it to mean you can put it on tape if you so desired.
LV objects can be tagged for administrative convenience. VGs and LVs can be made active as underlying devices become available through the use of the LVM ETAD demon.
I found that the setup again with CentOS is not as simple as with Linux Mint or Ubuntu and very different than Davian.
There wasn't like something that marches you through it. It was more like here's a menu and you have to click on each thing and do it and it was okay.
I just hadn't ever done it before and I don't have much experience with Anaconda and I have to stop that it could be a little more guided.
Like I said I had to click a little bit and for me with the 500 gigabyte or the 220 gigabyte disk or with the I'm sorry.
I had a 500 gigabyte minimum installed disk and it installed basically an open SSH server and no graphical anything and there was no nano.
So be ready if you're a nano guy there's no nano there and if you're behind a proxy you've got to do some stuff with the I so you'd better have your press I and escape from insert mode and colon W and colon and colon X all down because if not then it'll be a bad you'll have a bad day.
You really really really well. So I had to use vi until I got the packages and got my nano up on my thing and I know that there are those of you that don't think nano is an editor but I can do nano really really well and I'm very comfortable with it.
Whereas vi I have to think oh my god and of course every Linux certification test you better have vi vim little bit of emax going for you or there's going to be trouble.
I never a question about nano on any of them tests so but you can just do that.
As I said before it was just I got a bare server I got a bare system that had SSH on it and after I got the update issues fixed with the young config file.
I put good no moment and it was fine and as I said earlier the machine was having power issues and would not boot from the usb stick and so what I did was I had.
I burned the sent us image to a DVD at home and this was one of those modern portable blue ray DVD burners blue ray burner.
I put it on a DVDR and it worked you know we have some systems that are 15 years old 17 years old.
So I kept looking at the drive and I'd be like it's a DVD drive but it's a CD rider and I was like oh no so this might not work with the format.
So I had to go through four different drives in order to find one that would take a DVD are.
And because the power was weak I had to get another little desktop computer and just move the SATA cable from that desktop into the the integrap box and perform connect the power to that box turn on its power supply and use the DVD with the SATA connected to the other one to get sent to us to boot.
So if you can imagine I had the integrap box taken apart and I had to cover off another one and I took the SATA cable out and put the SATA cable in from the DVD into the integrap box and then use the drive to boot and install everything.
But after I did it it started to work and it started working just fine and of course like I said earlier for whatever reason the IDE drive wasn't working and I'm almost 100% that's a power supply or maybe something loose or not grounded or something.
So to remove the old sent to us six partitions I had to use G parted as when I went into the sent to us install.
I didn't I'm sure that's there somewhere but I didn't see a use the entire disk and erase everything option like I would in mint or Ubuntu or Davian and so I just said well I don't know so I'll use G parted and all which I learned a lot from because I saw that.
There was that extraction layer that talked about for LEM and then there was the XT well partitions underneath.
And so what is G parted G parted is a free partition editor for graphically managing your disk partitions which you parted you can resize copy and move partitions with our data loss.
Getting the merits to work through the firewall is pretty hard and I had to do two config changes to the young config one was the proxy address and the other was to allow young to do its own HTTP caching.
And so you also have to use the export underscore proxy equals command to get it globally so your SSH goes through the firewall from the command line after you install GNOME though you can use your company proxy script to get that to work.
And after some really frustrating times I was finally able to get it to have a fast mirror and I was able to install GNOME and I made the changes so that it would boot to the graphical login manager and GNOME when it booted up.
Note that it doesn't take any memory really until you actually log into the system so that I still had over 800 megabyte free after I did this so but I went from using only 64 or 68 megabytes to using 212 megabytes after I did the GNOME upgrade.
And eventually I'll install x2go or bnc server in case I need to access it for whatever reason I'm sure playing with the repos are doing a distro upgrade it will be easier via the graphical interface and be trying to do it for the first time from the command line.
So after I did its updates and I was sure that it could talk and everything to the outside world I went ahead and moved it back to the server room and I got an IP address and connected to it for putty and I think that the advantage of this box is always have a red hat 7 install ready to demo or learn something without having to set a lot of things up like images or trying to get something to boot or trying to get hyper V.
Trying to get hyper V to work when you've got a class or teaching something it's just really difficult you really have to and I don't have the experience with hyper V on my laptop to really get it to work.
And I did the move from VMware workstation to hyper V because with the workstation I kept on having to reinstall windows 10 partition a lot.
And so hopefully at some point I'll be able to buy a either a pixel book or a one of those high end laptop with with Susan and I can try KVM with it.
And well that's about it. That's at the end of my notes. I hope you enjoyed this. It was a little longer than I normally do. It was a pretty big project and it's still pretty fresh on my mind because I did it yesterday.
And if you have any questions please my email is at JWP5 at hotmail.com and thank you for listening to Hacker Public Radio.
God bless Ken Fallon and Dave Morris for all the work that they do and all the other contributors to and I hope that you'll have a great day.
Bye now.
You've been listening to Hacker Public Radio at HackerPublicRadio.org.
We are a community podcast network that releases shows every weekday Monday through Friday.
Today's show, like all our shows, was contributed by an HBR listener like yourself.
If you ever thought of recording a podcast then click on our contributing to find out how easy it really is.
Hacker Public Radio was founded by the digital dog pound and the infonomican computer club and is part of the binary revolution at binrev.com.
If you have comments on today's show, please email the host directly, leave a comment on the website or record a follow-up episode yourself.
Unless otherwise stated, today's show is released under Creative Commons, Attribution, Share a Life, 3.0 license.
